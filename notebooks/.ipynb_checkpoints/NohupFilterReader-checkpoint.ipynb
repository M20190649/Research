{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89cfc9ae-b81e-472e-a2d4-cfe93be58183",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ad593c-09d8-453d-acd5-15749db3dfb2",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.spark spark-sql_2.11 2.1.0\n",
    "org.apache.log4j.Logger.getRootLogger().setLevel(org.apache.log4j.Level.ERROR);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@694b6248"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder() \n",
    "  .master(\"local[*]\") \n",
    "  .config(\"spark.executor.memory\", \"3g\")\n",
    "  .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\")\n",
    "  .appName(\"NohupReader\")\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@7d0bb9f2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val research_home: String = scala.util.Properties.envOrElse(\"RESEARCH_HOME\", \"/home/acald013/Research/\")\n",
    "val folder = s\"${research_home}Scripts/Python/\"\n",
    "val prefix = \"nohup\"\n",
    "\n",
    "val nohup = spark.read.textFile(s\"${folder}${prefix}.out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(nohup.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Line\n",
       "defined class Run\n",
       "defined class Stage\n",
       "defined class MDFrow\n",
       "defined class DatasetOps\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Line(line: String, n: Long)\n",
    "case class Run(runID: Long, date: String, method: String, cores: Int, epsilon: Double, mu: Int, delta: Int, methodTime: Double)\n",
    "case class Stage(runID: Long, n: Long, timestamp: String, stage: String, stageTime: Double, load: Int, unit: String)\n",
    "case class MDFrow(mdfID: Long, n: Long, method: String, epsilon: Double, mu: Int, delta: Int, stage: String, time: Double, load: Int, unit: String)\n",
    "\n",
    "implicit class DatasetOps(ds: org.apache.spark.sql.Dataset[_]) {\n",
    "    def display(rows: Int = 20) = {\n",
    "        import com.twosigma.beakerx.scala.table.TableDisplay\n",
    "        val columns = ds.columns\n",
    "        val rowVals = ds.toDF.take(rows)\n",
    "        val t = new TableDisplay(rowVals map (row => (columns zip row.toSeq).toMap))\n",
    "        t.display()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                line|  n|\n",
      "+--------------------+---+\n",
      "|FLOCKFINDER=Merge...|  0|\n",
      "|WARNING:root:Sett...|  1|\n",
      "|acald013@dblab-ra...|  2|\n",
      "|acald013@dblab-ra...|  3|\n",
      "|acald013@dblab-ra...|  4|\n",
      "|acald013@dblab-ra...|  5|\n",
      "|no org.apache.spa...|  6|\n",
      "|starting org.apac...|  7|\n",
      "|acald013@dblab-ra...|  8|\n",
      "|acald013@dblab-ra...|  9|\n",
      "+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = nohup.toDF(\"line\").withColumn(\"n\", monotonicallyIncreasingId).as[Line].cache()\n",
    "val nLines = lines.count()\n",
    "lines.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|runID|n  |\n",
      "+-----+---+\n",
      "|0    |18 |\n",
      "|0    |19 |\n",
      "|0    |20 |\n",
      "|0    |21 |\n",
      "|0    |22 |\n",
      "|0    |23 |\n",
      "|0    |24 |\n",
      "|0    |25 |\n",
      "|0    |26 |\n",
      "|0    |27 |\n",
      "+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indicesRun = lines.filter{ l => \n",
    "        l.line.contains(\"=== MergeLast Start ===\") || l.line.contains(\"method=MergeLast,\") ||\n",
    "        l.line.contains(\"=== SpatialJoin Start ===\") || l.line.contains(\"method=SpatialJoin,\")\n",
    "    }\n",
    "    .orderBy(\"n\")\n",
    "    .select(\"n\")\n",
    "    .collect()\n",
    "    .toList\n",
    "    .map(_.getLong(0))\n",
    "    .grouped(2)\n",
    "    .toList\n",
    "    .map(pair => (pair.head, pair.last))\n",
    "    .filter(r => r._1 != r._2)\n",
    "    .zipWithIndex\n",
    "val indexRun = spark.createDataset(indicesRun)\n",
    "    .flatMap{ pair => \n",
    "        (pair._1._1 to pair._1._2)\n",
    "        .toList.map(v => (pair._2, v))\n",
    "    }\n",
    "    .toDF(\"runID\",\"n\")\n",
    "    .cache\n",
    "indexRun.show(10, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|runID|date                   |method   |cores|epsilon|mu |delta|methodTime|\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|0    |2018-06-07 22:22:17,016|MergeLast|28   |100.0  |4  |5    |927.875   |\n",
      "|1    |2018-06-08 10:40:22,043|MergeLast|28   |60.0   |4  |5    |654.964   |\n",
      "|2    |2018-06-08 10:52:07,203|MergeLast|28   |70.0   |4  |5    |686.385   |\n",
      "|3    |2018-06-08 11:03:42,503|MergeLast|28   |80.0   |4  |5    |677.862   |\n",
      "|4    |2018-06-08 11:17:24,874|MergeLast|28   |90.0   |4  |5    |798.868   |\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val runs = indexRun.join(lines, \"n\").\n",
    "    groupBy(\"runID\").\n",
    "    agg(max($\"n\").alias(\"n\")).\n",
    "    join(lines, \"n\").\n",
    "    select(\"runID\", \"line\").\n",
    "    orderBy(\"runID\").\n",
    "    map{ row =>\n",
    "        val runID = row.getInt(0)\n",
    "        val line  = row.getString(1)\n",
    "        var arr1  = line.split(\" -> \")\n",
    "        val date  = arr1(0)\n",
    "        val arr2  = arr1(1).split(\",\")\n",
    "        val method  = arr2(0).split(\"=\")(1)\n",
    "        val cores   = arr2(1).split(\"=\")(1).toInt\n",
    "        val epsilon = arr2(2).split(\"=\")(1).toDouble\n",
    "        val mu      = arr2(3).split(\"=\")(1).toInt\n",
    "        val delta   = arr2(4).split(\"=\")(1).toInt\n",
    "        val time    = arr2(5).split(\"=\")(1).toDouble\n",
    "        Run(runID, date, method, cores, epsilon, mu, delta, time)\n",
    "    }.\n",
    "    cache\n",
    "val nRuns = runs.count()\n",
    "runs.show(nRuns.toInt, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|runID|date                   |method   |cores|epsilon|mu |delta|methodTime|\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|0    |2018-06-07 22:22:17,016|MergeLast|28   |100.0  |4  |5    |927.875   |\n",
      "|1    |2018-06-08 10:40:22,043|MergeLast|28   |60.0   |4  |5    |654.964   |\n",
      "|2    |2018-06-08 10:52:07,203|MergeLast|28   |70.0   |4  |5    |686.385   |\n",
      "|3    |2018-06-08 11:03:42,503|MergeLast|28   |80.0   |4  |5    |677.862   |\n",
      "|4    |2018-06-08 11:17:24,874|MergeLast|28   |90.0   |4  |5    |798.868   |\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampleRuns = runs.filter($\"method\" === \"MergeLast\").filter($\"mu\" === 4 || $\"mu\" === 3).filter($\"delta\" === 3 || $\"delta\" === 5)\n",
    "val nSampleRuns = sampleRuns.count()\n",
    "sampleRuns.show(nSampleRuns.toInt, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure",
     "output_type": "error",
     "text": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 461, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1\n\tat $anonfun$2.apply(<console>:121)\n\tat $anonfun$2.apply(<console>:115)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:105)\n\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:97)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2405)\n  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2404)\n  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)\n  at org.apache.spark.sql.Dataset.count(Dataset.scala:2404)\n  ... 66 elided\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 1\n  at $anonfun$2.apply(<console>:121)\n  at $anonfun$2.apply(<console>:115)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:105)\n  at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:97)\n  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n  ... 3 more\n",
     "traceback": [
      "\u001b[1;31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 461, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 1\u001b[0;0m",
      "\u001b[1;31m\tat $anonfun$2.apply(<console>:121)\u001b[0;0m",
      "\u001b[1;31m\tat $anonfun$2.apply(<console>:115)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:105)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:97)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\u001b[0;0m",
      "\u001b[1;31m\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\u001b[0;0m",
      "\u001b[1;31m\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\u001b[0;0m",
      "\u001b[1;31m\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\u001b[0;0m",
      "\u001b[1;31m\tat java.lang.Thread.run(Thread.java:745)\u001b[0;0m",
      "\u001b[1;31m\u001b[0;0m",
      "\u001b[1;31mDriver stacktrace:\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\u001b[0;0m",
      "\u001b[1;31m  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\u001b[0;0m",
      "\u001b[1;31m  at scala.Option.foreach(Option.scala:257)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2405)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2404)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.Dataset.count(Dataset.scala:2404)\u001b[0;0m",
      "\u001b[1;31m  ... 66 elided\u001b[0;0m",
      "\u001b[1;31mCaused by: java.lang.ArrayIndexOutOfBoundsException: 1\u001b[0;0m",
      "\u001b[1;31m  at $anonfun$2.apply(<console>:121)\u001b[0;0m",
      "\u001b[1;31m  at $anonfun$2.apply(<console>:115)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:105)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1.next(InMemoryRelation.scala:97)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.scheduler.Task.run(Task.scala:99)\u001b[0;0m",
      "\u001b[1;31m  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\u001b[0;0m",
      "\u001b[1;31m  ... 3 more\u001b[0;0m"
     ]
    }
   ],
   "source": [
    "val stages = lines.filter(_.line.contains(\"|\")).\n",
    "    join(indexRun, \"n\").\n",
    "    map{ m =>\n",
    "        val n         = m.getLong(0)\n",
    "        val line      = m.getString(1)\n",
    "        val runID     = m.getInt(2)\n",
    "        var arr1      = line.split(\" -> \")\n",
    "        val timestamp = arr1(0).trim\n",
    "        val arr2      = arr1(1).split(\" \\\\| \")\n",
    "        val stage     = arr2(0).trim\n",
    "        val time      = arr2(1).trim.dropRight(1).toDouble\n",
    "        val arr3      = arr2(2).trim.split(\" \")\n",
    "        val load      = arr3(0).toInt\n",
    "        val unit      = arr3(1)\n",
    "        Stage(runID, n, timestamp, stage, time, load, unit)\n",
    "    }.\n",
    "    join(sampleRuns.select($\"runID\"), \"runID\").\n",
    "    cache\n",
    "    \n",
    "println(stages.count())\n",
    "stages.show(10, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+-------+---+-----+----------------------------+---------+\n",
      "|runID|n  |method   |epsilon|mu |delta|stage                       |stageTime|\n",
      "+-----+---+---------+-------+---+-----+----------------------------+---------+\n",
      "|1    |291|MergeLast|30.0   |4  |3    |0.Reporting locations       |5.63     |\n",
      "|1    |307|MergeLast|30.0   |4  |3    |1.Set of disks for t_i      |31.31    |\n",
      "|1    |308|MergeLast|30.0   |4  |3    |0.Reporting locations       |4.98     |\n",
      "|1    |324|MergeLast|30.0   |4  |3    |2.Set of disks for t_i+delta|20.51    |\n",
      "|1    |325|MergeLast|30.0   |4  |3    |3.Joining timestams         |8.55     |\n",
      "|1    |326|MergeLast|30.0   |4  |3    |4.Checking internals        |11.68    |\n",
      "|1    |327|MergeLast|30.0   |4  |3    |0.Reporting locations       |4.91     |\n",
      "|1    |343|MergeLast|30.0   |4  |3    |1.Set of disks for t_i      |19.58    |\n",
      "|1    |344|MergeLast|30.0   |4  |3    |0.Reporting locations       |4.93     |\n",
      "|1    |360|MergeLast|30.0   |4  |3    |2.Set of disks for t_i+delta|35.6     |\n",
      "|1    |361|MergeLast|30.0   |4  |3    |3.Joining timestams         |7.04     |\n",
      "|1    |362|MergeLast|30.0   |4  |3    |4.Checking internals        |9.44     |\n",
      "|1    |363|MergeLast|30.0   |4  |3    |0.Reporting locations       |4.89     |\n",
      "|1    |364|MergeLast|30.0   |4  |3    |1.Set of disks for t_i      |0.19     |\n",
      "|1    |365|MergeLast|30.0   |4  |3    |0.Reporting locations       |4.94     |\n",
      "|1    |381|MergeLast|30.0   |4  |3    |2.Set of disks for t_i+delta|34.83    |\n",
      "|1    |382|MergeLast|30.0   |4  |3    |3.Joining timestams         |7.26     |\n",
      "|1    |383|MergeLast|30.0   |4  |3    |4.Checking internals        |9.93     |\n",
      "|1    |384|MergeLast|30.0   |4  |3    |0.Reporting locations       |4.96     |\n",
      "|1    |385|MergeLast|30.0   |4  |3    |1.Set of disks for t_i      |0.19     |\n",
      "+-----+---+---------+-------+---+-----+----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ml_stages = stages.join(runs, \"runID\").\n",
    "    filter($\"method\" === \"MergeLast\").\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage\".alias(\"stage0\"), $\"stageTime\").\n",
    "    withColumn(\"stage1\", regexp_replace($\"stage0\", \"Reporting locations at t=\\\\d+\", \"0.Reporting locations\")).\n",
    "    withColumn(\"stage2\", regexp_replace($\"stage1\", \"Checking internal timestamps\", \"4.Checking internals\")).\n",
    "    withColumn(\"stage3\", regexp_replace($\"stage2\", \"\\\\.\\\\.\\\\.\", \"\")).\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage3\".alias(\"stage\"), $\"stageTime\").\n",
    "    filter(!$\"stage\".rlike(\"4.Distance Join phase\")).\n",
    "    filter(!$\"stage\".rlike(\"5.Getting candidates\"))\n",
    "ml_stages.show(truncate = false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------+---+-----+-----+---------+\n",
      "|runID|n  |method|epsilon|mu |delta|stage|stageTime|\n",
      "+-----+---+------+-------+---+-----+-----+---------+\n",
      "+-----+---+------+-------+---+-----+-----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sj_stages = stages.join(runs, \"runID\").\n",
    "    filter($\"method\" === \"SpatialJoin\").\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage\".alias(\"stage0\"), $\"stageTime\").\n",
    "    withColumn(\"stage1\", regexp_replace($\"stage0\", \"\\\\.\\\\.\\\\.\", \"\")).\n",
    "    withColumn(\"stage2\", regexp_replace($\"stage1\", \"Reporting\", \"0.Reporting\")).\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage2\".alias(\"stage\"), $\"stageTime\").\n",
    "    filter(!$\"stage\".rlike(\"4.Distance Join phase\")).\n",
    "    filter(!$\"stage\".rlike(\"5.Getting candidates\"))\n",
    "sj_stages.show(truncate = false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "+-----+---------+-------+---+-----+-------+\n",
      "|runID|method   |epsilon|mu |delta|time   |\n",
      "+-----+---------+-------+---+-----+-------+\n",
      "|1    |MergeLast|30.0   |4  |3    |592.782|\n",
      "|3    |MergeLast|30.0   |4  |3    |584.446|\n",
      "|5    |MergeLast|30.0   |4  |3    |593.125|\n",
      "|7    |MergeLast|30.0   |4  |3    |577.23 |\n",
      "|9    |MergeLast|30.0   |4  |3    |585.75 |\n",
      "|11   |MergeLast|30.0   |3  |5    |689.323|\n",
      "|16   |MergeLast|30.0   |3  |5    |668.433|\n",
      "|20   |MergeLast|30.0   |3  |5    |681.964|\n",
      "|24   |MergeLast|30.0   |3  |5    |689.051|\n",
      "|28   |MergeLast|30.0   |3  |5    |687.891|\n",
      "+-----+---------+-------+---+-----+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = runs.select($\"runID\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"methodTime\".alias(\"time\")).\n",
    "    filter($\"method\" === \"MergeLast\").filter($\"mu\" === 4 || $\"mu\" === 3).filter($\"delta\" === 3 || $\"delta\" === 5).\n",
    "    orderBy($\"runID\", $\"epsilon\", $\"method\").\n",
    "    cache\n",
    "println(data.count())\n",
    "data.show(data.count().toInt, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = data.collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}methods_${prefix}_draft.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = ml_stages.union(sj_stages).collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}stages_${prefix}_draft.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val indicesMdf = lines.filter{ l => \n",
    "        l.line.contains(\" -> Setting mu=\") || l.line.contains(\" ->   berlin0-10,\") \n",
    "    }\n",
    "    .orderBy(\"n\")\n",
    "    .select(\"n\")\n",
    "    .collect()\n",
    "    .toList\n",
    "    .map(_.getLong(0))\n",
    "    .grouped(2)\n",
    "    .toList\n",
    "    .map(pair => (pair.head, pair.last))\n",
    "    .filter(r => r._1 != r._2)\n",
    "    .zipWithIndex\n",
    "val indexMdf = spark.createDataset(indicesMdf)\n",
    "    .flatMap{ pair => \n",
    "        (pair._1._1 to pair._1._2)\n",
    "        .toList.map(v => (pair._2, v))\n",
    "    }\n",
    "    .toDF(\"mdfID\",\"n\")\n",
    "    .cache\n",
    "indexMdf.show(15, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val mdfInfo = indexMdf.groupBy($\"mdfID\").agg(max($\"n\").alias(\"m\")).orderBy($\"m\")\n",
    "mdfInfo.show(10, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val mdfInfo = indexMdf.groupBy($\"mdfID\").agg(max($\"n\").alias(\"n\")).orderBy($\"n\").\n",
    "    join(indexRun, \"n\").\n",
    "    join(runs, \"runID\").\n",
    "    join(lines, \"n\").\n",
    "    select($\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"line\").\n",
    "    withColumn(\"timestamp\", substring($\"line\", 122, 124)).\n",
    "    select($\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"timestamp\")\n",
    "\n",
    "mdfInfo.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val d = mdfInfo.collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}mdfInfo_${prefix}.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
