{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "//import $ivy.`sh.almond::almond-spark:0.3\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                            \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4a98fa02\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1555960667337\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "//import $ivy.`sh.almond::almond-spark:0.3\n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "import $ivy.`org.vegas-viz::vegas:0.3.10`\n",
    "\n",
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql._\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .config(\"spark.serializer\",classOf[KryoSerializer].getName)\n",
    "    .master(\"local[*]\").appName(\"Areal\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mLog\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mLog\u001b[39m] = [timestamp: string, duration: int ... 9 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Log(timestamp: String, duration: Int, appID: String, nodes: String, id: Int, worker: String, stage: String\n",
    "               , rddBlocks: Int, tasksCompleted: Int, tasksDuration: Double , tasksLoad: Double)\n",
    "val data = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"|\")\n",
    "    .csv(\"/home/acald013/Research/Validation/MultinodeScaleup/monitor.log\")\n",
    "    .map{ l => \n",
    "         //\"timestamp\", \"duration\", \"appID\", \"nodes\", \"id\", \"worker\", \"stage\", \"rddBlocks\", \"tasksCompleted\", \"tasksDuration\", \"tasksLoad\")\n",
    "        val timestamp = l.getString(0)\n",
    "        val duration = l.getString(1).toInt\n",
    "        val appID    = l.getString(2)\n",
    "        val nodes    = l.getString(3).toInt\n",
    "        val id       = l.getString(4).toInt\n",
    "        val worker   = l.getString(5)\n",
    "        val stage    = l.getString(6)\n",
    "        val rdds     = l.getString(7).toInt\n",
    "        val comp     = l.getString(8).toInt\n",
    "        val dura     = l.getString(9).toDouble\n",
    "        val load     = l.getString(10).toDouble\n",
    "        \n",
    "        (timestamp, duration, appID, nodes, id, worker, stage, rdds, comp, dura, load)\n",
    "    }.toDF(\"timestamp\", \"duration\", \"appID\", \"nodes\", \"id\", \"worker\", \"stage\", \"rddBlocks\", \"tasksCompleted\", \"tasksDuration\", \"tasksLoad\").as[Log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+-----+---+--------------------+--------------------+---------+--------------+-------------+---------+\n",
      "|           timestamp|duration|               appID|nodes| id|              worker|               stage|rddBlocks|tasksCompleted|tasksDuration|tasksLoad|\n",
      "+--------------------+--------+--------------------+-----+---+--------------------+--------------------+---------+--------------+-------------+---------+\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  2|169.235.27.135:33775|flatMap at MF.sca...|     1037|          1354|        20.96|   519.73|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  1|169.235.27.137:45856|flatMap at MF.sca...|     1056|          1358|        21.63|    556.5|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  0|169.235.27.134:40730|flatMap at MF.sca...|     1139|          1513|        24.74|   658.93|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  2|169.235.27.135:33775|count at MF.scala...|     1129|          1446|        21.34|   519.73|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  1|169.235.27.137:45856|count at MF.scala...|     1139|          1441|        22.05|    556.5|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  0|169.235.27.134:40730|count at MF.scala...|     1232|          1606|        25.12|   658.93|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  2|169.235.27.135:33775|count at MF.scala...|     1165|          1482|        21.57|   519.73|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  1|169.235.27.137:45856|count at MF.scala...|     1176|          1479|        22.26|    556.5|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  0|169.235.27.134:40730|count at MF.scala...|     1259|          1633|        25.32|   658.93|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  2|169.235.27.135:33775|count at MF.scala...|     1187|          1504|        21.77|   519.73|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  1|169.235.27.137:45856|count at MF.scala...|     1211|          1513|        22.46|    556.5|\n",
      "|2019-04-20 11:52:...|      33|app-2019042011522...|    3|  0|169.235.27.134:40730|count at MF.scala...|     1290|          1664|        25.55|   658.93|\n",
      "|2019-04-20 11:52:...|      34|app-2019042011522...|    3|  2|169.235.27.135:33775|count at MF.scala...|     1250|          1566|         22.1|   519.73|\n",
      "|2019-04-20 11:52:...|      34|app-2019042011522...|    3|  1|169.235.27.137:45856|count at MF.scala...|     1281|          1583|        22.78|    556.5|\n",
      "|2019-04-20 11:52:...|      34|app-2019042011522...|    3|  0|169.235.27.134:40730|count at MF.scala...|     1355|          1727|        25.87|   658.93|\n",
      "|2019-04-20 11:52:...|      34|app-2019042011522...|    3|  2|169.235.27.135:33775|count at MF.scala...|     1321|          1636|        22.34|   519.73|\n",
      "|2019-04-20 11:52:...|      34|app-2019042011522...|    3|  1|169.235.27.137:45856|count at MF.scala...|     1364|          1665|        23.09|    556.5|\n",
      "|2019-04-20 11:52:...|      34|app-2019042011522...|    3|  0|169.235.27.134:40730|count at MF.scala...|     1435|          1809|        26.11|   658.93|\n",
      "|2019-04-20 11:52:...|      34|app-2019042011522...|    3|  2|169.235.27.135:33775|count at MF.scala...|     1418|          1735|         22.7|   519.73|\n",
      "|2019-04-20 11:52:...|      34|app-2019042011522...|    3|  1|169.235.27.137:45856|count at MF.scala...|     1458|          1760|         23.4|    556.5|\n",
      "+--------------------+--------+--------------------+-----+---+--------------------+--------------------+---------+--------------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres22_0\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mLog\u001b[39m] = [timestamp: string, duration: int ... 9 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.persist(StorageLevel.MEMORY_ONLY)\n",
    "data.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|appID                  |\n",
      "+-----------------------+\n",
      "|app-20190420115223-0000|\n",
      "|app-20190420120928-0001|\n",
      "|app-20190420122227-0002|\n",
      "|app-20190420123748-0003|\n",
      "|app-20190420125022-0004|\n",
      "|app-20190420130734-0005|\n",
      "|app-20190420132031-0006|\n",
      "|app-20190420133750-0007|\n",
      "|app-20190420135120-0008|\n",
      "|app-20190420140717-0009|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mapps\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [appID: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val apps = data.select(\"appID\").distinct().orderBy(\"appID\").cache()\n",
    "apps.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mextractAppRates\u001b[39m"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractAppRates(appID: String, data: Dataset[Log]): DataFrame = {\n",
    "    val node_info = data.filter(_.appID == appID).select($\"duration\", $\"id\", $\"worker\", $\"rddBlocks\", $\"tasksCompleted\", $\"tasksLoad\")\n",
    "        .groupBy($\"duration\", $\"id\", $\"worker\")\n",
    "        .agg(mean($\"rddBlocks\").as(\"rdds\"), mean($\"tasksCompleted\").as(\"task\"), mean($\"tasksLoad\").as(\"load\"))\n",
    "        .withColumn(\"rddsRate\", $\"rdds\" / $\"duration\")\n",
    "        .withColumn(\"taskRate\", $\"task\" / $\"duration\")\n",
    "        .withColumn(\"loadRate\", $\"load\" / $\"duration\")\n",
    "        .select($\"duration\", $\"worker\", $\"rddsRate\", $\"taskRate\", $\"loadRate\")\n",
    "        .orderBy($\"duration\", $\"worker\")\n",
    "    val pw = new PrintWriter(s\"/tmp/${appID}.tsv\")\n",
    "    pw.write(node_info.map{ m => \n",
    "        s\"${m.getInt(0)}\\t${m.getString(1)}\\t${m.getDouble(2)}\\t${m.getDouble(3)}\\t${m.getDouble(4)}\\n\"\n",
    "    }.collect().mkString(\"\"))\n",
    "    pw.close()\n",
    "    \n",
    "    node_info\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------------------+--------------------+\n",
      "|duration|worker              |rddsRate            |taskRate           |loadRate            |\n",
      "+--------+--------------------+--------------------+-------------------+--------------------+\n",
      "|3       |169.235.27.134:40431|0.0                 |0.0                |0.0                 |\n",
      "|4       |169.235.27.134:40431|0.0                 |0.0                |0.0                 |\n",
      "|5       |169.235.27.134:40431|0.0                 |0.03333333333333333|0.0                 |\n",
      "|6       |169.235.27.134:40431|0.0                 |0.25396825396825395|0.050873015873015875|\n",
      "|7       |169.235.27.134:40431|0.006211180124223602|2.3540372670807455 |1.4332919254658378  |\n",
      "|8       |169.235.27.134:40431|1.5773809523809523  |4.738095238095238  |2.8251190476190464  |\n",
      "|9       |169.235.27.134:40431|3.587301587301587   |6.412698412698413  |4.258201058201059   |\n",
      "|10      |169.235.27.134:40431|4.747368421052632   |9.042105263157895  |6.840631578947369   |\n",
      "|11      |169.235.27.134:40431|4.790909090909091   |11.10909090909091  |8.626045454545455   |\n",
      "|12      |169.235.27.134:40431|6.504166666666666   |11.525             |8.287416666666667   |\n",
      "|13      |169.235.27.134:40431|7.932126696832579   |15.176470588235293 |21.64117647058824   |\n",
      "|14      |169.235.27.134:40431|8.869047619047619   |19.29365079365079  |28.048055555555568  |\n",
      "|15      |169.235.27.134:40431|8.65925925925926    |19.9037037037037   |27.19877777777778   |\n",
      "|16      |169.235.27.134:40431|8.466911764705882   |18.996323529411764 |26.286654411764715  |\n",
      "|17      |169.235.27.134:40431|8.30718954248366    |18.18300653594771  |25.723039215686278  |\n",
      "|18      |169.235.27.134:40431|7.944444444444445   |17.416666666666668 |25.111666666666675  |\n",
      "|19      |169.235.27.134:40431|7.526315789473684   |17.05263157894737  |23.79000000000001   |\n",
      "|20      |169.235.27.134:40431|7.15                |16.852777777777778 |22.600500000000007  |\n",
      "|21      |169.235.27.134:40431|7.345238095238095   |17.767857142857142 |22.192976190476195  |\n",
      "|22      |169.235.27.134:40431|7.412878787878788   |20.598484848484848 |25.424507575757573  |\n",
      "+--------+--------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"app-20190420123748-0003\"\u001b[39m\n",
       "\u001b[36mtm\u001b[39m: \u001b[32mDataFrame\u001b[39m = [duration: int, worker: string ... 3 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appID = \"app-20190420123748-0003\"\n",
    "val tm = extractAppRates(appID, data)\n",
    "tm.show(truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(app <- apps.collect()){\n",
    "    extractAppRates(app.getString(0), data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mextractAppMetrics\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractAppMetrics(appID: String, data: Dataset[Log]): DataFrame = {\n",
    "    val node_info = data.filter(f => f.appID == appID && f.worker.contains(\":\"))\n",
    "        .select($\"duration\", $\"worker\", $\"rddBlocks\", $\"tasksCompleted\", $\"tasksLoad\")\n",
    "        .groupBy($\"duration\", $\"worker\")\n",
    "        .agg(mean($\"rddBlocks\").as(\"rdds\"), mean($\"tasksCompleted\").as(\"task\"), mean($\"tasksLoad\").as(\"load\"))\n",
    "        .select($\"duration\", $\"worker\", $\"rdds\", $\"task\", $\"load\")\n",
    "        .orderBy($\"duration\", $\"worker\")\n",
    "    val fileID = appID.split(\"-\")(2)\n",
    "    val pw = new PrintWriter(s\"/tmp/M${fileID}.tsv\")\n",
    "    pw.write(node_info.map{ m => \n",
    "        s\"${m.getInt(0)}\\t${m.getString(1)}\\t${m.getInt(2)}\\t${m.getInt(3)}\\t${m.getDouble(4)}\\n\"\n",
    "    }.collect().mkString(\"\"))\n",
    "    pw.close()\n",
    "    \n",
    "    node_info\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
