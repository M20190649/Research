{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d36a3af-4687-4fbc-93ba-fd2c2be5fc87",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.spark spark-sql_2.11 2.1.0\n",
    "org.apache.log4j.Logger.getRootLogger().setLevel(org.apache.log4j.Level.ERROR);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@1025fb6f"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder() \n",
    "  .master(\"local[*]\") \n",
    "  .config(\"spark.executor.memory\", \"3g\")\n",
    "  .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\")\n",
    "  .appName(\"NohupReader\")\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@76553344"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val research_home: String = scala.util.Properties.envOrElse(\"RESEARCH_HOME\", \"/home/acald013/Research/\")\n",
    "val folder = s\"${research_home}Scripts/Python/\"\n",
    "val prefix = \"nohup\"\n",
    "\n",
    "val nohup = spark.read.textFile(s\"${folder}${prefix}*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(nohup.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Line\n",
       "defined class Run\n",
       "defined class Stage\n",
       "defined class MDFrow\n",
       "defined class DatasetOps\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Line(line: String, n: Long)\n",
    "case class Run(runID: Long, date: String, method: String, cores: Int, epsilon: Double, mu: Int, delta: Int, methodTime: Double)\n",
    "case class Stage(runID: Long, n: Long, timestamp: String, stage: String, stageTime: Double, load: Int, unit: String)\n",
    "case class MDFrow(mdfID: Long, n: Long, method: String, epsilon: Double, mu: Int, delta: Int, stage: String, time: Double, load: Int, unit: String)\n",
    "\n",
    "implicit class DatasetOps(ds: org.apache.spark.sql.Dataset[_]) {\n",
    "    def display(rows: Int = 20) = {\n",
    "        import com.twosigma.beakerx.scala.table.TableDisplay\n",
    "        val columns = ds.columns\n",
    "        val rowVals = ds.toDF.take(rows)\n",
    "        val t = new TableDisplay(rowVals map (row => (columns zip row.toSeq).toMap))\n",
    "        t.display()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca890d14-9131-4a4c-9658-697795a86458",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = nohup.toDF(\"line\").withColumn(\"n\", monotonicallyIncreasingId).as[Line].cache()\n",
    "val nLines = lines.count()\n",
    "lines.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8e11fb-ef12-471b-a9e6-3649535568a7",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indicesRun = lines.filter{ l => \n",
    "        l.line.contains(\"=== MergeLast Start ===\") || l.line.contains(\"method=MergeLast,\") ||\n",
    "        l.line.contains(\"=== SpatialJoin Start ===\") || l.line.contains(\"method=SpatialJoin,\")\n",
    "    }\n",
    "    .orderBy(\"n\")\n",
    "    .select(\"n\")\n",
    "    .collect()\n",
    "    .toList\n",
    "    .map(_.getLong(0))\n",
    "    .grouped(2)\n",
    "    .toList\n",
    "    .map(pair => (pair.head, pair.last))\n",
    "    .filter(r => r._1 != r._2)\n",
    "    .zipWithIndex\n",
    "val indexRun = spark.createDataset(indicesRun)\n",
    "    .flatMap{ pair => \n",
    "        (pair._1._1 to pair._1._2)\n",
    "        .toList.map(v => (pair._2, v))\n",
    "    }\n",
    "    .toDF(\"runID\",\"n\")\n",
    "    .cache\n",
    "indexRun.display(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2d9756-8018-4712-aab3-2578622e3d65",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val runs = indexRun.join(lines, \"n\").\n",
    "    groupBy(\"runID\").\n",
    "    agg(max($\"n\").alias(\"n\")).\n",
    "    join(lines, \"n\").\n",
    "    select(\"runID\", \"line\").\n",
    "    orderBy(\"runID\").\n",
    "    map{ row =>\n",
    "        val runID = row.getInt(0)\n",
    "        val line  = row.getString(1)\n",
    "        var arr1  = line.split(\" -> \")\n",
    "        val date  = arr1(0)\n",
    "        val arr2  = arr1(1).split(\",\")\n",
    "        val method  = arr2(0).split(\"=\")(1)\n",
    "        val cores   = arr2(1).split(\"=\")(1).toInt\n",
    "        val epsilon = arr2(2).split(\"=\")(1).toDouble\n",
    "        val mu      = arr2(3).split(\"=\")(1).toInt\n",
    "        val delta   = arr2(4).split(\"=\")(1).toInt\n",
    "        val time    = arr2(5).split(\"=\")(1).toDouble\n",
    "        Run(runID, date, method, cores, epsilon, mu, delta, time)\n",
    "    }.\n",
    "    cache\n",
    "runs.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc9f7cf-2764-4cc0-99fa-15b0670293f4",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stages = lines.filter(_.line.contains(\"|\")).\n",
    "    join(indexRun, \"n\").\n",
    "    map{ m =>\n",
    "        val n         = m.getLong(0)\n",
    "        val line      = m.getString(1)\n",
    "        val runID     = m.getInt(2)\n",
    "        var arr1      = line.split(\" -> \")\n",
    "        val timestamp = arr1(0).trim\n",
    "        val arr2      = arr1(1).split(\"\\\\|\")\n",
    "        val stage     = arr2(0).trim\n",
    "        val time      = arr2(1).trim.dropRight(1).toDouble\n",
    "        val arr3      = arr2(2).trim.split(\" \")\n",
    "        val load      = arr3(0).toInt\n",
    "        val unit      = arr3(1)\n",
    "        Stage(runID, n, timestamp, stage, time, load, unit)\n",
    "    }.cache\n",
    "    \n",
    "stages.count()\n",
    "stages.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+-------+---+-----+----------------------------+---------+\n",
      "|runID|n  |method   |epsilon|mu |delta|stage                       |stageTime|\n",
      "+-----+---+---------+-------+---+-----+----------------------------+---------+\n",
      "|0    |19 |MergeLast|10.0   |4  |5    |0.Reporting locations       |5.3      |\n",
      "|0    |35 |MergeLast|10.0   |4  |5    |1.Set of disks for t_i      |29.4     |\n",
      "|0    |36 |MergeLast|10.0   |4  |5    |0.Reporting locations       |5.01     |\n",
      "|0    |52 |MergeLast|10.0   |4  |5    |2.Set of disks for t_i+delta|32.2     |\n",
      "|0    |53 |MergeLast|10.0   |4  |5    |3.Joining timestams         |3.89     |\n",
      "|0    |117|MergeLast|10.0   |4  |5    |4.Checking internals        |57.76    |\n",
      "|0    |118|MergeLast|10.0   |4  |5    |0.Reporting locations       |4.76     |\n",
      "|0    |134|MergeLast|10.0   |4  |5    |1.Set of disks for t_i      |18.14    |\n",
      "|0    |135|MergeLast|10.0   |4  |5    |0.Reporting locations       |4.78     |\n",
      "|0    |151|MergeLast|10.0   |4  |5    |2.Set of disks for t_i+delta|30.3     |\n",
      "|0    |152|MergeLast|10.0   |4  |5    |3.Joining timestams         |3.4      |\n",
      "|0    |216|MergeLast|10.0   |4  |5    |4.Checking internals        |55.52    |\n",
      "|0    |217|MergeLast|10.0   |4  |5    |0.Reporting locations       |4.8      |\n",
      "|0    |233|MergeLast|10.0   |4  |5    |1.Set of disks for t_i      |17.57    |\n",
      "|0    |234|MergeLast|10.0   |4  |5    |0.Reporting locations       |4.8      |\n",
      "|0    |250|MergeLast|10.0   |4  |5    |2.Set of disks for t_i+delta|30.91    |\n",
      "|0    |251|MergeLast|10.0   |4  |5    |3.Joining timestams         |3.9      |\n",
      "|0    |315|MergeLast|10.0   |4  |5    |4.Checking internals        |52.87    |\n",
      "|0    |316|MergeLast|10.0   |4  |5    |0.Reporting locations       |4.81     |\n",
      "|0    |332|MergeLast|10.0   |4  |5    |1.Set of disks for t_i      |30.51    |\n",
      "+-----+---+---------+-------+---+-----+----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ml_stages = stages.join(runs, \"runID\").\n",
    "    filter($\"method\" === \"MergeLast\").\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage\".alias(\"stage0\"), $\"stageTime\").\n",
    "    withColumn(\"stage1\", regexp_replace($\"stage0\", \"Reporting locations at t=\\\\d+\", \"0.Reporting locations\")).\n",
    "    withColumn(\"stage2\", regexp_replace($\"stage1\", \"Checking internal timestamps\", \"4.Checking internals\")).\n",
    "    withColumn(\"stage3\", regexp_replace($\"stage2\", \"\\\\.\\\\.\\\\.\", \"\")).\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage3\".alias(\"stage\"), $\"stageTime\").\n",
    "    filter(!$\"stage\".rlike(\"4.Distance Join phase\")).\n",
    "    filter(!$\"stage\".rlike(\"5.Getting candidates\"))\n",
    "ml_stages.show(truncate = false)\n",
    "//ml_stages.display(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------+-------+---+-----+----------------------+---------+\n",
      "|runID|n   |method     |epsilon|mu |delta|stage                 |stageTime|\n",
      "+-----+----+-----------+-------+---+-----+----------------------+---------+\n",
      "|5    |3325|SpatialJoin|10.0   |4  |5    |0.Reporting locations |27.57    |\n",
      "|5    |3341|SpatialJoin|10.0   |4  |5    |1.Set of disks for t_i|141.36   |\n",
      "|5    |3342|SpatialJoin|10.0   |4  |5    |4.Found flocks        |0.41     |\n",
      "|5    |3343|SpatialJoin|10.0   |4  |5    |5.Updating times      |0.42     |\n",
      "|5    |3344|SpatialJoin|10.0   |4  |5    |6.Filter phase        |0.86     |\n",
      "|5    |3345|SpatialJoin|10.0   |4  |5    |0.Reporting locations |5.19     |\n",
      "|5    |3361|SpatialJoin|10.0   |4  |5    |1.Set of disks for t_i|20.51    |\n",
      "|5    |3362|SpatialJoin|10.0   |4  |5    |2.Distance Join phase |2.95     |\n",
      "|5    |3363|SpatialJoin|10.0   |4  |5    |3.Getting candidates  |1.55     |\n",
      "|5    |3364|SpatialJoin|10.0   |4  |5    |4.Found flocks        |0.2      |\n",
      "|5    |3365|SpatialJoin|10.0   |4  |5    |5.Updating times      |0.26     |\n",
      "|5    |3366|SpatialJoin|10.0   |4  |5    |6.Filter phase        |0.49     |\n",
      "|5    |3367|SpatialJoin|10.0   |4  |5    |0.Reporting locations |5.0      |\n",
      "|5    |3383|SpatialJoin|10.0   |4  |5    |1.Set of disks for t_i|19.7     |\n",
      "|5    |3384|SpatialJoin|10.0   |4  |5    |2.Distance Join phase |2.52     |\n",
      "|5    |3385|SpatialJoin|10.0   |4  |5    |3.Getting candidates  |1.3      |\n",
      "|5    |3386|SpatialJoin|10.0   |4  |5    |4.Found flocks        |0.26     |\n",
      "|5    |3387|SpatialJoin|10.0   |4  |5    |5.Updating times      |0.34     |\n",
      "|5    |3388|SpatialJoin|10.0   |4  |5    |6.Filter phase        |0.55     |\n",
      "|5    |3389|SpatialJoin|10.0   |4  |5    |0.Reporting locations |5.05     |\n",
      "+-----+----+-----------+-------+---+-----+----------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sj_stages = stages.join(runs, \"runID\").\n",
    "    filter($\"method\" === \"SpatialJoin\").\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage\".alias(\"stage0\"), $\"stageTime\").\n",
    "    withColumn(\"stage1\", regexp_replace($\"stage0\", \"\\\\.\\\\.\\\\.\", \"\")).\n",
    "    withColumn(\"stage2\", regexp_replace($\"stage1\", \"Reporting\", \"0.Reporting\")).\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage2\".alias(\"stage\"), $\"stageTime\").\n",
    "    filter(!$\"stage\".rlike(\"4.Distance Join phase\")).\n",
    "    filter(!$\"stage\".rlike(\"5.Getting candidates\"))\n",
    "sj_stages.show(truncate = false)\n",
    "//sj_stages.display(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed94aa9-7638-4871-ab64-5d308ab59d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = runs.select($\"runID\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"methodTime\".alias(\"time\")).\n",
    "    orderBy($\"runID\", $\"epsilon\", $\"method\").\n",
    "    cache\n",
    "data.count()\n",
    "data.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = data.collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}methods.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = ml_stages.union(sj_stages).collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}stages.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d665b2-12bc-406c-b30c-e845e574851e",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indicesMdf = lines.filter{ l => \n",
    "        l.line.contains(\" -> Setting mu=\") || l.line.contains(\" ->   berlin0-10,\") \n",
    "    }\n",
    "    .orderBy(\"n\")\n",
    "    .select(\"n\")\n",
    "    .collect()\n",
    "    .toList\n",
    "    .map(_.getLong(0))\n",
    "    .grouped(2)\n",
    "    .toList\n",
    "    .map(pair => (pair.head, pair.last))\n",
    "    .filter(r => r._1 != r._2)\n",
    "    .zipWithIndex\n",
    "val indexMdf = spark.createDataset(indicesMdf)\n",
    "    .flatMap{ pair => \n",
    "        (pair._1._1 to pair._1._2)\n",
    "        .toList.map(v => (pair._2, v))\n",
    "    }\n",
    "    .toDF(\"mdfID\",\"n\")\n",
    "    .cache\n",
    "indexMdf.display(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mdfID: int, t: int]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mdfInfo = indexMdf.groupBy($\"mdfID\").agg(max($\"n\").alias(\"m\")).orderBy($\"m\")\n",
    "val mdfInternalIDs = mdfInfo.join(lines, $\"m\" === $\"n\").\n",
    "    map{ m =>\n",
    "        val mdfID  = m.getInt(0)\n",
    "        val line   = m.getString(2).split(\" -> \")(1)\n",
    "        val t      = line.split(\",\").last.trim.toInt\n",
    "        (mdfID, t)\n",
    "    }.\n",
    "    toDF(\"mdfID\", \"t\").\n",
    "    filter($\"t\" < 0).\n",
    "    cache\n",
    "//mdfInternalIDs.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-------+---+-----+---------+---------------------------------+----+-----+\n",
      "|runID|mdfID|n  |epsilon|mu |delta|method   |stage                            |load|time |\n",
      "+-----+-----+---+-------+---+-----+---------+---------------------------------+----+-----+\n",
      "|0    |2    |57 |10.0   |4  |5    |MergeLast|A.Indexing points                |34  |3.311|\n",
      "|0    |2    |58 |10.0   |4  |5    |MergeLast|B.Getting pairs                  |56  |0.852|\n",
      "|0    |2    |59 |10.0   |4  |5    |MergeLast|C.Computing centers              |112 |0.483|\n",
      "|0    |2    |60 |10.0   |4  |5    |MergeLast|D.Indexing centers               |112 |0.706|\n",
      "|0    |2    |61 |10.0   |4  |5    |MergeLast|E.Getting disks                  |112 |1.364|\n",
      "|0    |2    |62 |10.0   |4  |5    |MergeLast|F.Filtering less-than-mu disks   |29  |0.261|\n",
      "|0    |2    |63 |10.0   |4  |5    |MergeLast|G.Prunning duplicate candidates  |13  |0.692|\n",
      "|0    |2    |64 |10.0   |4  |5    |MergeLast|H.Indexing candidates            |13  |1.149|\n",
      "|0    |2    |65 |10.0   |4  |5    |MergeLast|I.Getting expansions             |42  |0.161|\n",
      "|0    |2    |66 |10.0   |4  |5    |MergeLast|J.Finding maximal disks          |9   |0.193|\n",
      "|0    |2    |67 |10.0   |4  |5    |MergeLast|K.Prunning duplicates and subsets|8   |0.618|\n",
      "|0    |3    |78 |10.0   |4  |5    |MergeLast|A.Indexing points                |34  |3.57 |\n",
      "|0    |3    |79 |10.0   |4  |5    |MergeLast|B.Getting pairs                  |55  |1.077|\n",
      "|0    |3    |80 |10.0   |4  |5    |MergeLast|C.Computing centers              |110 |1.013|\n",
      "|0    |3    |81 |10.0   |4  |5    |MergeLast|D.Indexing centers               |110 |0.754|\n",
      "|0    |3    |82 |10.0   |4  |5    |MergeLast|E.Getting disks                  |110 |1.761|\n",
      "|0    |3    |83 |10.0   |4  |5    |MergeLast|F.Filtering less-than-mu disks   |25  |0.312|\n",
      "|0    |3    |84 |10.0   |4  |5    |MergeLast|G.Prunning duplicate candidates  |11  |0.747|\n",
      "|0    |3    |85 |10.0   |4  |5    |MergeLast|H.Indexing candidates            |11  |1.255|\n",
      "|0    |3    |86 |10.0   |4  |5    |MergeLast|I.Getting expansions             |30  |0.182|\n",
      "+-----+-----+---+-------+---+-----+---------+---------------------------------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mdfInternals = lines.join(indexMdf, \"n\").\n",
    "    join(mdfInternalIDs, \"mdfID\").\n",
    "    filter($\"line\".rlike(\"[A-K]\\\\.\")).\n",
    "    map{ m =>\n",
    "        val mdfID = m.getInt(0)\n",
    "        val n = m.getLong(1)\n",
    "        val line = m.getString(2).split(\" -> \")(1)\n",
    "        val arr =  line.split(\"\\\\[\")\n",
    "        val stage = arr(0).trim\n",
    "        val time = arr(1).trim.split(\"s\")(0).toDouble\n",
    "        val load = arr(2).trim.split(\" \")(0).toInt\n",
    "        (mdfID, n, stage, time, load)\n",
    "    }.\n",
    "    toDF(\"mdfID\", \"n\", \"stage\", \"time\", \"load\").\n",
    "    orderBy(\"n\").join(indexRun, \"n\").\n",
    "    join(runs, \"runID\").\n",
    "    withColumn(\"stage2\", regexp_replace($\"stage\", \"\\\\.\\\\.\\\\.\", \"\")).\n",
    "    select($\"runID\", $\"mdfID\", $\"n\", $\"epsilon\", $\"mu\", $\"delta\", $\"method\", $\"stage2\".alias(\"stage\"), $\"load\", $\"time\").\n",
    "    cache\n",
    "mdfInternals.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = mdfInternals.collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}mdfInternals.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---+-----+---------+\n",
      "|method   |epsilon|mu |delta|timestamp|\n",
      "+---------+-------+---+-----+---------+\n",
      "|MergeLast|10.0   |4  |5    |  0      |\n",
      "|MergeLast|10.0   |4  |5    |  4      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    |  1      |\n",
      "|MergeLast|10.0   |4  |5    |  5      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    |  2      |\n",
      "|MergeLast|10.0   |4  |5    |  6      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    |  3      |\n",
      "|MergeLast|10.0   |4  |5    |  7      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "|MergeLast|10.0   |4  |5    | -1      |\n",
      "+---------+-------+---+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mdfInfo = indexMdf.groupBy($\"mdfID\").agg(max($\"n\").alias(\"n\")).orderBy($\"n\").\n",
    "    join(indexRun, \"n\").\n",
    "    join(runs, \"runID\").\n",
    "    join(lines, \"n\").\n",
    "    select($\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"line\").\n",
    "    withColumn(\"timestamp\", substring($\"line\", 122, 124)).\n",
    "    select($\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"timestamp\")\n",
    "\n",
    "mdfInfo.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = mdfInfo.collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}mdfInfo.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
