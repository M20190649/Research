# Install multiverse repository in apt...
deb http://us.archive.ubuntu.com/ubuntu/ bionic multiverse

# Install python-pip, pssh and parallel-pssh ...
sudo apt install python-pip pssh
pip install parallel-ssh

# Set security group with AllTCP and ALLUDP among member of the same security group...

# Clone the repository...
git clone https://github.com/aocalderon/Research.git

# Download JDK...
wget http://www.cs.ucr.edu/~acald013/public/tmp/jdk.tar.gz
tar -zxvf jdk.tar.gz
echo "export JAVA_HOME=/home/ubuntu/jdk1.8.0_202" >> ~/.bashrc
echo "export PATH=\$PATH:\$JAVA_HOME/bin" >> ~/.bashrc

# Download SBT...
wget https://piccolo.link/sbt-1.2.8.tgz
tar -zxvf sbt-1.2.8.tgz
echo "export SBT_HOME=/home/ubuntu/sbt" >> ~/.bashrc
echo "export PATH=\$PATH:\$SBT_HOME/bin" >> ~/.bashrc

# Download Spark...
mkdir Spark
cd Spark
wget http://www.cs.ucr.edu/~acald013/public/tmp/spark.tar.gz
tar -zxvf spark.tar.gz
echo "export SPARK_HOME=/home/ubuntu/Spark/2.4" >> ~/.bashrc
echo "export PATH=\$PATH:\$SPARK_HOME/bin" >> ~/.bashrc

# Reload environment variables...
source ~/.bashrc

# Change content of spark-env.sh in Master accordingly...
emacs /home/ubuntu/Spark/2.4/conf/spark-env.sh

#export JAVA_HOME=/home/ubuntu/jdk1.8.0_202
#export SPARK_HOME=/home/ubuntu/Spark/spark-2.1.0-bin-hadoop2.7
#export SPARK_MASTER_HOST=_THE_MASTER_IP_
#export SPARK_MASTER_PORT=7077
#export SPARK_LOCAL_IP=_THE_MASTER_IP_

# Copy PEM file in Driver and set as Identity File in ~/.ssh/config ...
Host *
    IdentityFile ~/aws.pem

# Set /etc/hosts with the IPs of the worker and the driver

# Copy hosts file to workers...
for i in {1..15}; do scp /etc/hosts ubuntu@worker${i}:/home/ubuntu; done
parallel-ssh -h workers "sudo mv /home/ubuntu/hosts /etc/hosts"
    
# Back in driver copy JDK, Spark and Research folders to each worker...
rsync -avz Research/ ubuntu@_WORKER_HOST_:/home/ubuntu/Research/
rsync -avz jdk1.8.0_202/ ubuntu@_WORKER_HOST_:/home/ubuntu/jdk1.8.0_202/
rsync -avz Spark/ ubuntu@_WORKER_HOST_:/home/ubuntu/Spark/

# Change SPARK_MASTER_HOST and SPARK_LOCAL_IP in each worker ...
parallel-ssh -h workers "/home/ubuntu/Research/Docs/setWorker.sh"
for i in {1..15}; do echo "export SPARK_LOCAL_IP=worker${i}" >> /home/ubuntu/Spark/2.4/conf/spark-env.sh; done


# In Master don't forget to add the worker hosts to $SPARK_HOME/conf/slaves
echo "ubuntu@_WORKER_NUMBER_" >> /home/ubuntu/Spark/2.4/conf/slaves
