{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                             \n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` \n",
    "import $ivy.`InitialDLab:simba_2.11:1.0`\n",
    "import $ivy.`org.slf4j:slf4j-jdk14:1.7.25`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.slf4j.{Logger, LoggerFactory}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.simba.{Dataset, SimbaSession}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.simba.index.RTreeType\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{StructType, StructField, LongType, DoubleType, IntegerType}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.mutable.ListBuffer\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.slf4j.{Logger, LoggerFactory}\n",
    "import org.apache.spark.sql.simba.{Dataset, SimbaSession}\n",
    "import org.apache.spark.sql.simba.index.RTreeType\n",
    "import org.apache.spark.sql.types.{StructType, StructField, LongType, DoubleType, IntegerType}\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.collection.JavaConverters._\n",
    "import scala.collection.mutable.ListBuffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/acald013/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/acald013/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-jdk14/1.7.25/slf4j-jdk14-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "18/05/10 18:02:38 INFO SparkContext: Running Spark version 2.1.0\n",
      "18/05/10 18:02:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "18/05/10 18:02:39 INFO SecurityManager: Changing view acls to: acald013\n",
      "18/05/10 18:02:39 INFO SecurityManager: Changing modify acls to: acald013\n",
      "18/05/10 18:02:39 INFO SecurityManager: Changing view acls groups to: \n",
      "18/05/10 18:02:39 INFO SecurityManager: Changing modify acls groups to: \n",
      "18/05/10 18:02:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()\n",
      "18/05/10 18:02:39 INFO Utils: Successfully started service 'sparkDriver' on port 36974.\n",
      "18/05/10 18:02:39 INFO SparkEnv: Registering MapOutputTracker\n",
      "18/05/10 18:02:39 INFO SparkEnv: Registering BlockManagerMaster\n",
      "18/05/10 18:02:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "18/05/10 18:02:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "18/05/10 18:02:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-709562dc-d609-423d-93f9-12692065ea68\n",
      "18/05/10 18:02:39 INFO MemoryStore: MemoryStore started with capacity 8.2 GB\n",
      "18/05/10 18:02:39 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "18/05/10 18:02:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "18/05/10 18:02:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040\n",
      "18/05/10 18:02:39 INFO Executor: Starting executor ID driver on host localhost\n",
      "18/05/10 18:02:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38157.\n",
      "18/05/10 18:02:39 INFO NettyBlockTransferService: Server created on 169.235.27.138:38157\n",
      "18/05/10 18:02:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "18/05/10 18:02:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 38157, None)\n",
      "18/05/10 18:02:39 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:38157 with 8.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 38157, None)\n",
      "18/05/10 18:02:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 38157, None)\n",
      "18/05/10 18:02:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 38157, None)\n",
      "18/05/10 18:02:40 INFO SharedState: Warehouse path is 'file:/home/acald013/notebooks/spark-warehouse/'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msimba\u001b[39m: \u001b[32mSimbaSession\u001b[39m = org.apache.spark.sql.simba.SimbaSession@5f4c9561"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark://169.235.27.134:7077\n",
    "val simba = SimbaSession.builder()\n",
    "      .master(\"local[10]\")\n",
    "      .appName(\"MapMaximalDisks\")\n",
    "      .config(\"simba.index.partitions\", 1024)\n",
    "      .config(\"spark.cores.max\", 32)\n",
    "      .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mST_Point\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mFlock\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class ST_Point(id: Long, x: Double, y: Double, t: Int = -1)\n",
    "case class Flock(start: Int, end: Int, ids: String, x: Double = 0.0, y: Double = 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msimba.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36msimba.simbaImplicits._\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import simba.implicits._\n",
    "import simba.simbaImplicits._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mid\u001b[39m: \u001b[32mStructField\u001b[39m = StructField(id,LongType,true)\n",
       "\u001b[36mx\u001b[39m: \u001b[32mStructField\u001b[39m = StructField(x,DoubleType,true)\n",
       "\u001b[36my\u001b[39m: \u001b[32mStructField\u001b[39m = StructField(y,DoubleType,true)\n",
       "\u001b[36mt\u001b[39m: \u001b[32mStructField\u001b[39m = StructField(t,IntegerType,true)\n",
       "\u001b[36mpoint_schema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  StructField(id,LongType,true),\n",
       "  StructField(x,DoubleType,true),\n",
       "  StructField(y,DoubleType,true),\n",
       "  StructField(t,IntegerType,true)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val id = StructField(\"id\", LongType)\n",
    "val x = StructField(\"x\", DoubleType)\n",
    "val y = StructField(\"y\", DoubleType)\n",
    "val t = StructField(\"t\", IntegerType)\n",
    "val point_schema = StructType(Array(id, x, y, t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfilename\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/Research/Datasets/Berlin/berlin0-2.tsv\"\u001b[39m\n",
       "\u001b[36mpointset\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32mST_Point\u001b[39m] = [id: bigint, x: double ... 2 more fields]\n",
       "\u001b[36mnPointset\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m54732L\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filename = \"/home/acald013/Research/Datasets/Berlin/berlin0-2.tsv\"\n",
    "\n",
    "val pointset = simba.read\n",
    "      .option(\"header\", \"false\")\n",
    "      .option(\"sep\", \"\\t\")\n",
    "      .schema(point_schema)\n",
    "      .csv(filename)\n",
    "      .as[ST_Point]\n",
    "      .cache()\n",
    "val nPointset = pointset.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mgetMaximalDisks\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getMaximalDisks(pointset: Dataset[ST_Point], t: Int, fraction: Double) = {\n",
    "    pointset.filter(_.t == t).sample(false, fraction).map(d => Flock(t,t,s\"${d.id}\",d.x,d.y))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmaximalDisksCache\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mmutable\u001b[39m.\u001b[32mMap\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mDataset\u001b[39m[\u001b[32mFlock\u001b[39m]] = \u001b[33mMap\u001b[39m(\n",
       "  \u001b[32m1\u001b[39m -> [start: int, end: int ... 3 more fields],\n",
       "  \u001b[32m0\u001b[39m -> [start: int, end: int ... 3 more fields]\n",
       ")\n",
       "\u001b[36mt\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m1\u001b[39m\n",
       "\u001b[36mres14_2\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mFlock\u001b[39m] = [start: int, end: int ... 3 more fields]\n",
       "\u001b[36mres14_4\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mFlock\u001b[39m] = [start: int, end: int ... 3 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maximalDisksCache = collection.mutable.Map[Int, Dataset[Flock]]()\n",
    "var t = 0\n",
    "maximalDisksCache.getOrElseUpdate(t, getMaximalDisks(pointset, t, 0.025))\n",
    "t = 1\n",
    "maximalDisksCache.getOrElseUpdate(t, getMaximalDisks(pointset, t, 0.050))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mA\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mFlock\u001b[39m] = [start: int, end: int ... 3 more fields]\n",
       "\u001b[36mB\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mFlock\u001b[39m] = [start: int, end: int ... 3 more fields]\n",
       "\u001b[36mres15_4\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m508L\u001b[39m\n",
       "\u001b[36mres15_5\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1328L\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 0\n",
    "val A = maximalDisksCache.getOrElseUpdate(t, getMaximalDisks(pointset, t, 0.050))\n",
    "t = 2\n",
    "val B = maximalDisksCache.getOrElseUpdate(t, getMaximalDisks(pointset, t, 0.075))\n",
    "\n",
    "A.count()\n",
    "B.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---------+---------+\n",
      "|start|end|ids  |x        |y        |\n",
      "+-----+---+-----+---------+---------+\n",
      "|1    |1  |3808 |34570.076|23723.145|\n",
      "|1    |1  |17395|19542.262|26040.434|\n",
      "|1    |1  |14915|15320.889|14510.076|\n",
      "|1    |1  |7477 |23533.218|22633.317|\n",
      "|1    |1  |19056|21128.158|23570.095|\n",
      "|1    |1  |10247|17599.327|16705.207|\n",
      "|1    |1  |11699|21240.955|17517.009|\n",
      "|1    |1  |10060|19269.585|16530.147|\n",
      "|1    |1  |8325 |21550.966|20418.627|\n",
      "|1    |1  |6205 |22997.345|21205.563|\n",
      "|1    |1  |15119|16871.333|24260.321|\n",
      "|1    |1  |5948 |24582.866|22205.247|\n",
      "|1    |1  |14672|28662.935|12673.63 |\n",
      "|1    |1  |1873 |32135.954|18351.013|\n",
      "|1    |1  |16886|25886.479|21541.753|\n",
      "|1    |1  |18971|31648.662|12679.064|\n",
      "|1    |1  |15831|35610.404|12927.242|\n",
      "|1    |1  |18302|17390.862|13923.548|\n",
      "|1    |1  |8164 |20682.912|15859.364|\n",
      "|1    |1  |3782 |23722.257|12412.957|\n",
      "+-----+---+-----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maximalDisksCache.apply(1).show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simba.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
