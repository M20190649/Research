{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eacb155-81e1-45d8-b27b-0f4ab107e0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.spark spark-sql_2.11 2.1.0\n",
    "org.apache.log4j.Logger.getRootLogger().setLevel(org.apache.log4j.Level.ERROR);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@6054f1d1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder() \n",
    "  .master(\"local[*]\") \n",
    "  .config(\"spark.executor.memory\", \"3g\")\n",
    "  .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\")\n",
    "  .appName(\"NohupReader\")\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@486ffd9c"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val research_home: String = scala.util.Properties.envOrElse(\"RESEARCH_HOME\", \"/home/acald013/Research/\")\n",
    "val folder = s\"${research_home}Scripts/Python/data/\"\n",
    "val prefix = \"FF\"\n",
    "\n",
    "val nohup = spark.read.textFile(s\"${folder}${prefix}*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(nohup.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Line\n",
       "defined class Param\n",
       "defined class Stat\n",
       "defined class Record\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Line(line: String, n: Long)\n",
    "case class Param(runID: Long, date: String, method: String, cores: Int, epsilon: Double, mu: Int, delta: Int, methodTime: Double)\n",
    "case class Stat(runID: Long, n: Long, timestamp: String, stage: String, stageTime: Double, load: Int, unit: String)\n",
    "case class Record(runID: Long, date: String, method: String, cores: Int, epsilon: Double, mu: Int, delta: Int, methodTime: Double, n: Long, timestamp: String, stage: String, stageTime: Double, load: Int, unit: String)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23595"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = nohup.toDF(\"line\").withColumn(\"n\", monotonicallyIncreasingId).as[Line].cache()\n",
    "val nLines = lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[runID: int, n: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indices = lines.filter{ l => \n",
    "        l.line.contains(\"=== MergeLast Start ===\") || l.line.contains(\"method=MergeLast,\") ||\n",
    "        l.line.contains(\"=== SpatialJoin Start ===\") || l.line.contains(\"method=SpatialJoin,\")\n",
    "    }\n",
    "    .orderBy(\"n\")\n",
    "    .select(\"n\")\n",
    "    .collect()\n",
    "    .toList\n",
    "    .map(_.getLong(0))\n",
    "    .grouped(2)\n",
    "    .toList\n",
    "    .map(pair => (pair.head, pair.last))\n",
    "    .filter(r => r._1 != r._2)\n",
    "    .zipWithIndex\n",
    "val index = spark.createDataset(indices)\n",
    "    .flatMap{ pair => \n",
    "        (pair._1._1 to pair._1._2)\n",
    "        .toList.map(v => (pair._2, v))\n",
    "    }\n",
    "    .toDF(\"runID\",\"n\")\n",
    "    .cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DatasetOps\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit class DatasetOps(ds: org.apache.spark.sql.Dataset[_]) {\n",
    "    def display(rows: Int = 20) = {\n",
    "        // I do not understand why this import is necessary\n",
    "        import com.twosigma.beakerx.scala.table.TableDisplay\n",
    "        val columns = ds.columns\n",
    "        val rowVals = ds.toDF.take(rows)\n",
    "        val t = new TableDisplay(rowVals map (row => (columns zip row.toSeq).toMap))\n",
    "        t.display()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cfe693-a893-4239-ac9c-b7f4c9fef2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val runs = index.join(lines, \"n\").cache\n",
    "runs.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f488de-ae0f-4165-b43a-93ed0e8249cf",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val params = runs.groupBy(\"runID\")\n",
    "    .agg(max($\"n\").alias(\"n\"))\n",
    "    .join(lines, \"n\")\n",
    "    .select(\"runID\", \"line\")\n",
    "    .orderBy(\"runID\")\n",
    "    .map{ row =>\n",
    "        val runID = row.getInt(0)\n",
    "        val line  = row.getString(1)\n",
    "        var arr1  = line.split(\" -> \")\n",
    "        val date  = arr1(0)\n",
    "        val arr2  = arr1(1).split(\",\")\n",
    "        val method  = arr2(0).split(\"=\")(1)\n",
    "        val cores   = arr2(1).split(\"=\")(1).toInt\n",
    "        val epsilon = arr2(2).split(\"=\")(1).toDouble\n",
    "        val mu      = arr2(3).split(\"=\")(1).toInt\n",
    "        val delta   = arr2(4).split(\"=\")(1).toInt\n",
    "        val time    = arr2(5).split(\"=\")(1).toDouble\n",
    "        Param(runID, date, method, cores, epsilon, mu, delta, time)\n",
    "    }\n",
    "    .cache\n",
    "params.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------------------+------------------------------+---------+----+------------+\n",
      "|runID|n  |timestamp              |stage                         |stageTime|load|unit        |\n",
      "+-----+---+-----------------------+------------------------------+---------+----+------------+\n",
      "|0    |59 |2018-05-14 17:59:40,065|1.Getting disks               |346.49   |1537|disks       |\n",
      "|0    |60 |2018-05-14 17:59:59,741|2.Joining timestams           |19.68    |916 |candidates  |\n",
      "|0    |79 |2018-05-14 18:00:43,354|3.Distance Join phase...      |10.96    |4189|combinations|\n",
      "|0    |80 |2018-05-14 18:00:57,985|4.Getting candidates...       |14.63    |661 |candidates  |\n",
      "|0    |100|2018-05-14 18:01:40,221|3.Distance Join phase...      |10.1     |1953|combinations|\n",
      "|0    |101|2018-05-14 18:01:52,285|4.Getting candidates...       |12.06    |661 |candidates  |\n",
      "|0    |121|2018-05-14 18:02:53,315|3.Distance Join phase...      |13.44    |1933|combinations|\n",
      "|0    |122|2018-05-14 18:03:06,276|4.Getting candidates...       |12.96    |661 |candidates  |\n",
      "|0    |124|2018-05-14 18:03:08,345|5.Checking internal timestamps|15.03    |661 |flocks      |\n",
      "|0    |155|2018-05-14 18:08:30,015|1.Getting disks               |320.5    |1552|disks       |\n",
      "+-----+---+-----------------------+------------------------------+---------+----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stats = runs.filter(_.getString(2).contains(\"|\"))\n",
    "    .map{ row =>\n",
    "        val n     = row.getLong(0)\n",
    "        val runID = row.getInt(1)\n",
    "        val line  = row.getString(2)\n",
    "        var arr1  = line.split(\" -> \")\n",
    "        val timestamp  = arr1(0).trim\n",
    "        val arr2  = arr1(1).split(\"\\\\|\")\n",
    "        val stage = arr2(0).trim\n",
    "        val time  = arr2(1).trim.dropRight(1).toDouble\n",
    "        val arr3  = arr2(2).trim.split(\" \")\n",
    "        val load  = arr3(0).toInt\n",
    "        val unit  = arr3(1)\n",
    "        Stat(runID, n, timestamp, stage, time, load, unit)\n",
    "    }.cache\n",
    "stats.count()\n",
    "stats.show(10, truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268a1137-868c-4396-9bf1-1b27d4f9a029",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = params.join(stats, \"runID\").orderBy(\"epsilon\", \"method\").cache\n",
    "data.count()\n",
    "data.display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = data.collect.sortBy(_.getLong(8)).map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}output.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
