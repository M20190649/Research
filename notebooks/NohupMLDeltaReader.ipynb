{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6fa853-b51d-48df-a330-bb4ba0a3c0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93243ca0-7b0a-449e-a62f-4d885261b906",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.spark spark-sql_2.11 2.1.0\n",
    "org.apache.log4j.Logger.getRootLogger().setLevel(org.apache.log4j.Level.ERROR);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.sql._\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@1ce00fc2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder() \n",
    "  .master(\"local[*]\") \n",
    "  .config(\"spark.executor.memory\", \"3g\")\n",
    "  .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\")\n",
    "  .appName(\"NohupReader\")\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession$implicits$@32600583"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val research_home: String = scala.util.Properties.envOrElse(\"RESEARCH_HOME\", \"/home/acald013/Research/\")\n",
    "val folder = s\"${research_home}Scripts/Python/\"\n",
    "val prefix = \"delta\"\n",
    "\n",
    "val nohup = spark.read.textFile(s\"${folder}${prefix}.out\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(nohup.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Line\n",
       "defined class Run\n",
       "defined class Stage\n",
       "defined class MDFrow\n",
       "defined class DatasetOps\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Line(line: String, n: Long)\n",
    "case class Run(runID: Long, date: String, method: String, cores: Int, epsilon: Double, mu: Int, delta: Int, methodTime: Double)\n",
    "case class Stage(runID: Long, n: Long, timestamp: String, stage: String, stageTime: Double, load: Int, unit: String)\n",
    "case class MDFrow(mdfID: Long, n: Long, method: String, epsilon: Double, mu: Int, delta: Int, stage: String, time: Double, load: Int, unit: String)\n",
    "\n",
    "implicit class DatasetOps(ds: org.apache.spark.sql.Dataset[_]) {\n",
    "    def display(rows: Int = 20) = {\n",
    "        import com.twosigma.beakerx.scala.table.TableDisplay\n",
    "        val columns = ds.columns\n",
    "        val rowVals = ds.toDF.take(rows)\n",
    "        val t = new TableDisplay(rowVals map (row => (columns zip row.toSeq).toMap))\n",
    "        t.display()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                line|  n|\n",
      "+--------------------+---+\n",
      "|FLOCKFINDER=Merge...|  0|\n",
      "|WARNING:root:Sett...|  1|\n",
      "|acald013@dblab-ra...|  2|\n",
      "|acald013@dblab-ra...|  3|\n",
      "|acald013@dblab-ra...|  4|\n",
      "|acald013@dblab-ra...|  5|\n",
      "|stopping org.apac...|  6|\n",
      "|starting org.apac...|  7|\n",
      "|acald013@dblab-ra...|  8|\n",
      "|acald013@dblab-ra...|  9|\n",
      "+--------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = nohup.toDF(\"line\").withColumn(\"n\", monotonicallyIncreasingId).as[Line].cache()\n",
    "val nLines = lines.count()\n",
    "lines.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|runID|n  |\n",
      "+-----+---+\n",
      "|0    |18 |\n",
      "|0    |19 |\n",
      "|0    |20 |\n",
      "|0    |21 |\n",
      "|0    |22 |\n",
      "|0    |23 |\n",
      "|0    |24 |\n",
      "|0    |25 |\n",
      "|0    |26 |\n",
      "|0    |27 |\n",
      "+-----+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indicesRun = lines.filter{ l => \n",
    "        l.line.contains(\"=== MergeLast Start ===\") || l.line.contains(\"method=MergeLast,\") ||\n",
    "        l.line.contains(\"=== SpatialJoin Start ===\") || l.line.contains(\"method=SpatialJoin,\")\n",
    "    }\n",
    "    .orderBy(\"n\")\n",
    "    .select(\"n\")\n",
    "    .collect()\n",
    "    .toList\n",
    "    .map(_.getLong(0))\n",
    "    .grouped(2)\n",
    "    .toList\n",
    "    .map(pair => (pair.head, pair.last))\n",
    "    .filter(r => r._1 != r._2)\n",
    "    .zipWithIndex\n",
    "val indexRun = spark.createDataset(indicesRun)\n",
    "    .flatMap{ pair => \n",
    "        (pair._1._1 to pair._1._2)\n",
    "        .toList.map(v => (pair._2, v))\n",
    "    }\n",
    "    .toDF(\"runID\",\"n\")\n",
    "    .cache\n",
    "indexRun.show(10, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|runID|date                   |method   |cores|epsilon|mu |delta|methodTime|\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|0    |2018-06-13 11:02:58,298|MergeLast|28   |100.0  |5  |3    |1049.356  |\n",
      "|1    |2018-06-13 11:19:42,867|MergeLast|28   |100.0  |5  |4    |972.869   |\n",
      "|2    |2018-06-13 11:33:13,339|MergeLast|28   |100.0  |5  |5    |787.414   |\n",
      "|3    |2018-06-13 11:46:20,318|MergeLast|28   |100.0  |5  |6    |764.147   |\n",
      "|4    |2018-06-13 11:57:46,508|MergeLast|28   |100.0  |5  |7    |666.686   |\n",
      "|5    |2018-06-13 12:13:32,500|MergeLast|28   |100.0  |5  |3    |923.694   |\n",
      "|6    |2018-06-13 12:30:17,335|MergeLast|28   |100.0  |5  |4    |975.485   |\n",
      "|7    |2018-06-13 12:43:23,014|MergeLast|28   |100.0  |5  |5    |767.372   |\n",
      "|8    |2018-06-13 12:56:20,024|MergeLast|28   |100.0  |5  |6    |753.504   |\n",
      "|9    |2018-06-13 13:07:42,197|MergeLast|28   |100.0  |5  |7    |664.331   |\n",
      "|10   |2018-06-13 13:23:04,982|MergeLast|28   |100.0  |5  |3    |900.313   |\n",
      "|11   |2018-06-13 13:42:23,522|MergeLast|28   |100.0  |5  |4    |1127.38   |\n",
      "|12   |2018-06-13 13:55:19,515|MergeLast|28   |100.0  |5  |5    |757.413   |\n",
      "|13   |2018-06-13 14:07:33,525|MergeLast|28   |100.0  |5  |6    |716.208   |\n",
      "|14   |2018-06-13 14:18:19,603|MergeLast|28   |100.0  |5  |7    |623.224   |\n",
      "|15   |2018-06-13 14:34:13,346|MergeLast|28   |100.0  |5  |3    |930.73    |\n",
      "|16   |2018-06-13 14:53:59,098|MergeLast|28   |100.0  |5  |4    |1155.261  |\n",
      "|17   |2018-06-13 15:07:23,279|MergeLast|28   |100.0  |5  |5    |781.745   |\n",
      "|18   |2018-06-13 15:19:41,490|MergeLast|28   |100.0  |5  |6    |720.524   |\n",
      "|19   |2018-06-13 15:30:40,464|MergeLast|28   |100.0  |5  |7    |636.438   |\n",
      "|20   |2018-06-13 15:45:55,884|MergeLast|28   |100.0  |5  |3    |897.761   |\n",
      "|21   |2018-06-13 16:05:56,217|MergeLast|28   |100.0  |5  |4    |1172.196  |\n",
      "|22   |2018-06-13 16:19:49,167|MergeLast|28   |100.0  |5  |5    |814.932   |\n",
      "|23   |2018-06-13 16:32:27,233|MergeLast|28   |100.0  |5  |6    |739.8     |\n",
      "|24   |2018-06-13 16:43:50,858|MergeLast|28   |100.0  |5  |7    |665.983   |\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val runs = indexRun.join(lines, \"n\").\n",
    "    groupBy(\"runID\").\n",
    "    agg(max($\"n\").alias(\"n\")).\n",
    "    join(lines, \"n\").\n",
    "    select(\"runID\", \"line\").\n",
    "    orderBy(\"runID\").\n",
    "    map{ row =>\n",
    "        val runID = row.getInt(0)\n",
    "        val line  = row.getString(1)\n",
    "        var arr1  = line.split(\" -> \")\n",
    "        val date  = arr1(0)\n",
    "        val arr2  = arr1(1).split(\",\")\n",
    "        val method  = arr2(0).split(\"=\")(1)\n",
    "        val cores   = arr2(1).split(\"=\")(1).toInt\n",
    "        val epsilon = arr2(2).split(\"=\")(1).toDouble\n",
    "        val mu      = arr2(3).split(\"=\")(1).toInt\n",
    "        val delta   = arr2(4).split(\"=\")(1).toInt\n",
    "        val time    = arr2(5).split(\"=\")(1).toDouble\n",
    "        Run(runID, date, method, cores, epsilon, mu, delta, time)\n",
    "    }.\n",
    "    cache\n",
    "val nRuns = runs.count()\n",
    "runs.show(nRuns.toInt, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|runID|date                   |method   |cores|epsilon|mu |delta|methodTime|\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|0    |2018-06-13 11:02:58,298|MergeLast|28   |100.0  |5  |3    |1049.356  |\n",
      "|1    |2018-06-13 11:19:42,867|MergeLast|28   |100.0  |5  |4    |972.869   |\n",
      "|2    |2018-06-13 11:33:13,339|MergeLast|28   |100.0  |5  |5    |787.414   |\n",
      "|3    |2018-06-13 11:46:20,318|MergeLast|28   |100.0  |5  |6    |764.147   |\n",
      "|4    |2018-06-13 11:57:46,508|MergeLast|28   |100.0  |5  |7    |666.686   |\n",
      "|5    |2018-06-13 12:13:32,500|MergeLast|28   |100.0  |5  |3    |923.694   |\n",
      "|6    |2018-06-13 12:30:17,335|MergeLast|28   |100.0  |5  |4    |975.485   |\n",
      "|7    |2018-06-13 12:43:23,014|MergeLast|28   |100.0  |5  |5    |767.372   |\n",
      "|8    |2018-06-13 12:56:20,024|MergeLast|28   |100.0  |5  |6    |753.504   |\n",
      "|9    |2018-06-13 13:07:42,197|MergeLast|28   |100.0  |5  |7    |664.331   |\n",
      "|10   |2018-06-13 13:23:04,982|MergeLast|28   |100.0  |5  |3    |900.313   |\n",
      "|11   |2018-06-13 13:42:23,522|MergeLast|28   |100.0  |5  |4    |1127.38   |\n",
      "|12   |2018-06-13 13:55:19,515|MergeLast|28   |100.0  |5  |5    |757.413   |\n",
      "|13   |2018-06-13 14:07:33,525|MergeLast|28   |100.0  |5  |6    |716.208   |\n",
      "|14   |2018-06-13 14:18:19,603|MergeLast|28   |100.0  |5  |7    |623.224   |\n",
      "|15   |2018-06-13 14:34:13,346|MergeLast|28   |100.0  |5  |3    |930.73    |\n",
      "|16   |2018-06-13 14:53:59,098|MergeLast|28   |100.0  |5  |4    |1155.261  |\n",
      "|17   |2018-06-13 15:07:23,279|MergeLast|28   |100.0  |5  |5    |781.745   |\n",
      "|18   |2018-06-13 15:19:41,490|MergeLast|28   |100.0  |5  |6    |720.524   |\n",
      "|19   |2018-06-13 15:30:40,464|MergeLast|28   |100.0  |5  |7    |636.438   |\n",
      "|20   |2018-06-13 15:45:55,884|MergeLast|28   |100.0  |5  |3    |897.761   |\n",
      "|21   |2018-06-13 16:05:56,217|MergeLast|28   |100.0  |5  |4    |1172.196  |\n",
      "|22   |2018-06-13 16:19:49,167|MergeLast|28   |100.0  |5  |5    |814.932   |\n",
      "|23   |2018-06-13 16:32:27,233|MergeLast|28   |100.0  |5  |6    |739.8     |\n",
      "|24   |2018-06-13 16:43:50,858|MergeLast|28   |100.0  |5  |7    |665.983   |\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampleRuns = runs.filter($\"method\" === \"MergeLast\")\n",
    "val nSampleRuns = sampleRuns.count()\n",
    "sampleRuns.show(nSampleRuns.toInt, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1775\n",
      "+-----+---+-----------------------+----------------------------------+---------+-----+----------+\n",
      "|runID|n  |timestamp              |stage                             |stageTime|load |unit      |\n",
      "+-----+---+-----------------------+----------------------------------+---------+-----+----------+\n",
      "|0    |19 |2018-06-13 10:45:34,734|Reporting locations at t=0...     |5.55     |18093|points    |\n",
      "|0    |35 |2018-06-13 10:46:33,175|1.Set of disks for t_i...         |58.44    |2026 |disks     |\n",
      "|0    |36 |2018-06-13 10:46:38,100|Reporting locations at t=2...     |4.93     |18394|points    |\n",
      "|0    |52 |2018-06-13 10:47:25,863|2.Set of disks for t_i+delta...   |47.76    |2117 |disks     |\n",
      "|0    |53 |2018-06-13 10:47:45,529|3.Joining timestams               |19.67    |313  |candidates|\n",
      "|0    |54 |2018-06-13 10:47:59,609|Indexing candidates...            |1.2      |314  |candidates|\n",
      "|0    |55 |2018-06-13 10:47:59,827|Getting expansions...             |0.22     |4    |expansions|\n",
      "|0    |56 |2018-06-13 10:48:00,171|Finding local maximals...         |0.34     |307  |local     |\n",
      "|0    |57 |2018-06-13 10:48:02,877|Prunning duplicates and subsets...|2.71     |307  |flocks    |\n",
      "|0    |164|2018-06-13 10:48:04,280|Checking internal timestamps      |18.75    |307  |flocks    |\n",
      "|0    |165|2018-06-13 10:48:10,220|Reporting locations at t=1...     |5.1      |18245|points    |\n",
      "|0    |181|2018-06-13 10:48:46,666|1.Set of disks for t_i...         |36.45    |2049 |disks     |\n",
      "|0    |182|2018-06-13 10:48:51,635|Reporting locations at t=3...     |4.97     |18548|points    |\n",
      "|0    |198|2018-06-13 10:49:41,030|2.Set of disks for t_i+delta...   |49.4     |2166 |disks     |\n",
      "|0    |199|2018-06-13 10:49:58,701|3.Joining timestams               |17.67    |321  |candidates|\n",
      "|0    |200|2018-06-13 10:50:10,940|Indexing candidates...            |1.03     |321  |candidates|\n",
      "|0    |201|2018-06-13 10:50:11,124|Getting expansions...             |0.18     |4    |expansions|\n",
      "|0    |202|2018-06-13 10:50:12,765|Finding local maximals...         |1.64     |314  |local     |\n",
      "|0    |203|2018-06-13 10:50:14,423|Prunning duplicates and subsets...|1.66     |314  |flocks    |\n",
      "|0    |310|2018-06-13 10:50:15,758|Checking internal timestamps      |17.06    |314  |flocks    |\n",
      "+-----+---+-----------------------+----------------------------------+---------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stages = lines.filter(_.line.contains(\"| \")).\n",
    "    join(indexRun, \"n\").\n",
    "    map{ m =>\n",
    "        val n         = m.getLong(0)\n",
    "        val line      = m.getString(1)\n",
    "        val runID     = m.getInt(2)\n",
    "        var arr1      = line.split(\" -> \")\n",
    "        val timestamp = arr1(0).trim\n",
    "        val arr2      = arr1(1).split(\"\\\\|\")\n",
    "        val stage     = arr2(0).trim\n",
    "        val time      = arr2(1).trim.dropRight(1).toDouble\n",
    "        val arr3      = arr2(2).trim.split(\" \")\n",
    "        val load      = arr3(0).toInt\n",
    "        val unit      = arr3(1)\n",
    "        Stage(runID, n, timestamp, stage, time, load, unit)\n",
    "    }.\n",
    "    join(sampleRuns.select($\"runID\"), \"runID\").\n",
    "    cache\n",
    "    \n",
    "println(stages.count())\n",
    "stages.show(20, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+-------+---+-----+----------------------------+---------+\n",
      "|runID|n  |method   |epsilon|mu |delta|stage                       |stageTime|\n",
      "+-----+---+---------+-------+---+-----+----------------------------+---------+\n",
      "|0    |19 |MergeLast|100.0  |5  |3    |0.Reporting locations       |5.55     |\n",
      "|0    |35 |MergeLast|100.0  |5  |3    |1.Set of disks for t_i      |58.44    |\n",
      "|0    |36 |MergeLast|100.0  |5  |3    |0.Reporting locations       |4.93     |\n",
      "|0    |52 |MergeLast|100.0  |5  |3    |2.Set of disks for t_i+delta|47.76    |\n",
      "|0    |53 |MergeLast|100.0  |5  |3    |3.Joining timestams         |19.67    |\n",
      "|0    |164|MergeLast|100.0  |5  |3    |4.Checking internals        |18.75    |\n",
      "|0    |165|MergeLast|100.0  |5  |3    |0.Reporting locations       |5.1      |\n",
      "|0    |181|MergeLast|100.0  |5  |3    |1.Set of disks for t_i      |36.45    |\n",
      "|0    |182|MergeLast|100.0  |5  |3    |0.Reporting locations       |4.97     |\n",
      "|0    |198|MergeLast|100.0  |5  |3    |2.Set of disks for t_i+delta|49.4     |\n",
      "|0    |199|MergeLast|100.0  |5  |3    |3.Joining timestams         |17.67    |\n",
      "|0    |310|MergeLast|100.0  |5  |3    |4.Checking internals        |17.06    |\n",
      "|0    |311|MergeLast|100.0  |5  |3    |0.Reporting locations       |4.92     |\n",
      "|0    |312|MergeLast|100.0  |5  |3    |1.Set of disks for t_i      |0.32     |\n",
      "|0    |313|MergeLast|100.0  |5  |3    |0.Reporting locations       |4.87     |\n",
      "|0    |329|MergeLast|100.0  |5  |3    |2.Set of disks for t_i+delta|56.69    |\n",
      "|0    |330|MergeLast|100.0  |5  |3    |3.Joining timestams         |18.79    |\n",
      "|0    |441|MergeLast|100.0  |5  |3    |4.Checking internals        |20.83    |\n",
      "|0    |442|MergeLast|100.0  |5  |3    |0.Reporting locations       |4.87     |\n",
      "|0    |443|MergeLast|100.0  |5  |3    |1.Set of disks for t_i      |0.33     |\n",
      "+-----+---+---------+-------+---+-----+----------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ml_stages = stages.join(runs, \"runID\").\n",
    "    filter($\"method\" === \"MergeLast\").\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage\".alias(\"stage0\"), $\"stageTime\").\n",
    "    withColumn(\"stage1\", regexp_replace($\"stage0\", \"Reporting locations at t=\\\\d+\", \"0.Reporting locations\")).\n",
    "    withColumn(\"stage2\", regexp_replace($\"stage1\", \"Checking internal timestamps\", \"4.Checking internals\")).\n",
    "    withColumn(\"stage3\", regexp_replace($\"stage2\", \"\\\\.\\\\.\\\\.\", \"\")).\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage3\".alias(\"stage\"), $\"stageTime\").\n",
    "    filter(!$\"stage\".rlike(\"4.Distance Join phase\")).\n",
    "    filter(!$\"stage\".rlike(\"5.Getting candidates\")).\n",
    "    filter(!$\"stage\".rlike(\"Indexing candidates\")).\n",
    "    filter(!$\"stage\".rlike(\"Getting expansions\")).\n",
    "    filter(!$\"stage\".rlike(\"Finding local maximals\")).\n",
    "    filter(!$\"stage\".rlike(\"Prunning duplicates and subsets\"))\n",
    "ml_stages.show(truncate = false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+-------+---+-----+-----+---------+\n",
      "|runID|n  |method|epsilon|mu |delta|stage|stageTime|\n",
      "+-----+---+------+-------+---+-----+-----+---------+\n",
      "+-----+---+------+-------+---+-----+-----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sj_stages = stages.join(runs, \"runID\").\n",
    "    filter($\"method\" === \"SpatialJoin\").\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage\".alias(\"stage0\"), $\"stageTime\").\n",
    "    withColumn(\"stage1\", regexp_replace($\"stage0\", \"\\\\.\\\\.\\\\.\", \"\")).\n",
    "    withColumn(\"stage2\", regexp_replace($\"stage1\", \"Reporting\", \"0.Reporting\")).\n",
    "    select($\"runID\", $\"n\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"stage2\".alias(\"stage\"), $\"stageTime\").\n",
    "    filter(!$\"stage\".rlike(\"4.Distance Join phase\")).\n",
    "    filter(!$\"stage\".rlike(\"5.Getting candidates\"))\n",
    "sj_stages.show(truncate = false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "+-----+---------+-------+---+-----+--------+\n",
      "|runID|method   |epsilon|mu |delta|time    |\n",
      "+-----+---------+-------+---+-----+--------+\n",
      "|0    |MergeLast|100.0  |5  |3    |1049.356|\n",
      "|1    |MergeLast|100.0  |5  |4    |972.869 |\n",
      "|2    |MergeLast|100.0  |5  |5    |787.414 |\n",
      "|3    |MergeLast|100.0  |5  |6    |764.147 |\n",
      "|4    |MergeLast|100.0  |5  |7    |666.686 |\n",
      "|5    |MergeLast|100.0  |5  |3    |923.694 |\n",
      "|6    |MergeLast|100.0  |5  |4    |975.485 |\n",
      "|7    |MergeLast|100.0  |5  |5    |767.372 |\n",
      "|8    |MergeLast|100.0  |5  |6    |753.504 |\n",
      "|9    |MergeLast|100.0  |5  |7    |664.331 |\n",
      "|10   |MergeLast|100.0  |5  |3    |900.313 |\n",
      "|11   |MergeLast|100.0  |5  |4    |1127.38 |\n",
      "|12   |MergeLast|100.0  |5  |5    |757.413 |\n",
      "|13   |MergeLast|100.0  |5  |6    |716.208 |\n",
      "|14   |MergeLast|100.0  |5  |7    |623.224 |\n",
      "|15   |MergeLast|100.0  |5  |3    |930.73  |\n",
      "|16   |MergeLast|100.0  |5  |4    |1155.261|\n",
      "|17   |MergeLast|100.0  |5  |5    |781.745 |\n",
      "|18   |MergeLast|100.0  |5  |6    |720.524 |\n",
      "|19   |MergeLast|100.0  |5  |7    |636.438 |\n",
      "|20   |MergeLast|100.0  |5  |3    |897.761 |\n",
      "|21   |MergeLast|100.0  |5  |4    |1172.196|\n",
      "|22   |MergeLast|100.0  |5  |5    |814.932 |\n",
      "|23   |MergeLast|100.0  |5  |6    |739.8   |\n",
      "|24   |MergeLast|100.0  |5  |7    |665.983 |\n",
      "+-----+---------+-------+---+-----+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = runs.select($\"runID\", $\"method\", $\"epsilon\", $\"mu\", $\"delta\", $\"methodTime\".alias(\"time\")).\n",
    "    filter($\"method\" === \"MergeLast\").\n",
    "    orderBy($\"runID\", $\"epsilon\", $\"method\").\n",
    "    cache\n",
    "println(data.count())\n",
    "data.show(data.count().toInt, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = data.collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}ml_${prefix}.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = ml_stages.union(sj_stages).collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}ml_${prefix}_stages.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------------------------------------+-------+---+-----+------+-----+\n",
      "|n  |runID|stage                               |epsilon|mu |delta|time  |load |\n",
      "+---+-----+------------------------------------+-------+---+-----+------+-----+\n",
      "|21 |0    |A.Indexing points...                |100.0  |5  |3    |8.869 |18093|\n",
      "|22 |0    |B.Getting pairs...                  |100.0  |5  |3    |8.56  |33957|\n",
      "|23 |0    |C.Computing centers...              |100.0  |5  |3    |2.602 |67914|\n",
      "|24 |0    |D.Indexing centers...               |100.0  |5  |3    |2.186 |67914|\n",
      "|25 |0    |E.Getting disks...                  |100.0  |5  |3    |8.118 |67914|\n",
      "|26 |0    |F.Filtering less-than-mu disks...   |100.0  |5  |3    |1.183 |22371|\n",
      "|27 |0    |G.Prunning duplicate candidates...  |100.0  |5  |3    |3.593 |13241|\n",
      "|28 |0    |H.Indexing candidates...            |100.0  |5  |3    |4.188 |13241|\n",
      "|29 |0    |I.Getting expansions...             |100.0  |5  |3    |0.489 |39014|\n",
      "|30 |0    |J.Finding maximal disks...          |100.0  |5  |3    |19.615|2060 |\n",
      "|31 |0    |K.Prunning duplicates and subsets...|100.0  |5  |3    |2.185 |2026 |\n",
      "|38 |0    |A.Indexing points...                |100.0  |5  |3    |4.872 |18394|\n",
      "|39 |0    |B.Getting pairs...                  |100.0  |5  |3    |5.902 |34918|\n",
      "|40 |0    |C.Computing centers...              |100.0  |5  |3    |1.773 |69836|\n",
      "|41 |0    |D.Indexing centers...               |100.0  |5  |3    |1.484 |69836|\n",
      "|42 |0    |E.Getting disks...                  |100.0  |5  |3    |6.878 |69836|\n",
      "|43 |0    |F.Filtering less-than-mu disks...   |100.0  |5  |3    |0.593 |23121|\n",
      "|44 |0    |G.Prunning duplicate candidates...  |100.0  |5  |3    |2.936 |13638|\n",
      "|45 |0    |H.Indexing candidates...            |100.0  |5  |3    |3.364 |13638|\n",
      "|46 |0    |I.Getting expansions...             |100.0  |5  |3    |0.322 |38445|\n",
      "+---+-----+------------------------------------+-------+---+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mdf = lines.filter($\"line\".rlike(\"-> [A-K]\\\\.\"))\n",
    "    .map{ m =>\n",
    "        val line  = m.line.split(\" -> \")(1)\n",
    "        val arr   = line.split(\"\\\\[\")\n",
    "        val stage = arr(0).trim\n",
    "        val time  = arr(1).trim.split(\"s\")(0).toDouble\n",
    "        val r = arr(2).trim.split(\" \")(0)\n",
    "        \n",
    "        (stage, time, r, m.n)\n",
    "    }\n",
    "    .toDF(\"stage\", \"time\", \"load\", \"n\")\n",
    "    .join(indexRun, \"n\")\n",
    "    .join(runs, \"runID\")\n",
    "    .select(\"n\", \"runID\", \"stage\", \"epsilon\", \"mu\", \"delta\", \"time\", \"load\")\n",
    "mdf.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|mdfID|n  |\n",
      "+-----+---+\n",
      "|0    |20 |\n",
      "|0    |21 |\n",
      "|0    |22 |\n",
      "|0    |23 |\n",
      "|0    |24 |\n",
      "|0    |25 |\n",
      "|0    |26 |\n",
      "|0    |27 |\n",
      "|0    |28 |\n",
      "|0    |29 |\n",
      "|0    |30 |\n",
      "|0    |31 |\n",
      "|0    |32 |\n",
      "|0    |33 |\n",
      "|1    |37 |\n",
      "+-----+---+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indicesMdf = lines.filter{ l => \n",
    "        l.line.contains(\" -> Setting mu=\") || l.line.contains(\" ->   berlin0-10,\") \n",
    "    }\n",
    "    .orderBy(\"n\")\n",
    "    .select(\"n\")\n",
    "    .collect()\n",
    "    .toList\n",
    "    .map(_.getLong(0))\n",
    "    .grouped(2)\n",
    "    .toList\n",
    "    .map(pair => (pair.head, pair.last))\n",
    "    .filter(r => r._1 != r._2)\n",
    "    .zipWithIndex\n",
    "val indexMdf = spark.createDataset(indicesMdf)\n",
    "    .flatMap{ pair => \n",
    "        (pair._1._1 to pair._1._2)\n",
    "        .toList.map(v => (pair._2, v))\n",
    "    }\n",
    "    .toDF(\"mdfID\",\"n\")\n",
    "    .cache\n",
    "indexMdf.show(15, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|mdfID|timestamp|\n",
      "+-----+---------+\n",
      "|0    |  0      |\n",
      "|1    |  2      |\n",
      "|2    |  1      |\n",
      "|3    |  3      |\n",
      "|4    |  4      |\n",
      "|5    |  5      |\n",
      "|6    |  6      |\n",
      "|7    |  7      |\n",
      "|8    |  8      |\n",
      "|9    |  9      |\n",
      "|10   | 10      |\n",
      "|11   |  0      |\n",
      "|12   |  3      |\n",
      "|13   |  1      |\n",
      "|14   |  4      |\n",
      "|15   |  2      |\n",
      "|16   |  5      |\n",
      "|17   |  6      |\n",
      "|18   |  7      |\n",
      "|19   |  8      |\n",
      "+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mdfInfo = indexMdf.groupBy($\"mdfID\").agg(max($\"n\").alias(\"n\")).orderBy($\"n\").\n",
    "    join(lines, \"n\").\n",
    "    withColumn(\"timestamp\", substring($\"line\", 122, 124)).\n",
    "    select($\"mdfID\", $\"timestamp\")\n",
    "mdfInfo.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+------------------------------------+-------+---+-----+------+-----+---------+\n",
      "|mdfID|n  |runID|stage                               |epsilon|mu |delta|time  |load |timestamp|\n",
      "+-----+---+-----+------------------------------------+-------+---+-----+------+-----+---------+\n",
      "|0    |21 |0    |A.Indexing points...                |100.0  |5  |3    |8.869 |18093|  0      |\n",
      "|0    |22 |0    |B.Getting pairs...                  |100.0  |5  |3    |8.56  |33957|  0      |\n",
      "|0    |23 |0    |C.Computing centers...              |100.0  |5  |3    |2.602 |67914|  0      |\n",
      "|0    |24 |0    |D.Indexing centers...               |100.0  |5  |3    |2.186 |67914|  0      |\n",
      "|0    |25 |0    |E.Getting disks...                  |100.0  |5  |3    |8.118 |67914|  0      |\n",
      "|0    |26 |0    |F.Filtering less-than-mu disks...   |100.0  |5  |3    |1.183 |22371|  0      |\n",
      "|0    |27 |0    |G.Prunning duplicate candidates...  |100.0  |5  |3    |3.593 |13241|  0      |\n",
      "|0    |28 |0    |H.Indexing candidates...            |100.0  |5  |3    |4.188 |13241|  0      |\n",
      "|0    |29 |0    |I.Getting expansions...             |100.0  |5  |3    |0.489 |39014|  0      |\n",
      "|0    |30 |0    |J.Finding maximal disks...          |100.0  |5  |3    |19.615|2060 |  0      |\n",
      "|0    |31 |0    |K.Prunning duplicates and subsets...|100.0  |5  |3    |2.185 |2026 |  0      |\n",
      "|1    |38 |0    |A.Indexing points...                |100.0  |5  |3    |4.872 |18394|  2      |\n",
      "|1    |39 |0    |B.Getting pairs...                  |100.0  |5  |3    |5.902 |34918|  2      |\n",
      "|1    |40 |0    |C.Computing centers...              |100.0  |5  |3    |1.773 |69836|  2      |\n",
      "|1    |41 |0    |D.Indexing centers...               |100.0  |5  |3    |1.484 |69836|  2      |\n",
      "|1    |42 |0    |E.Getting disks...                  |100.0  |5  |3    |6.878 |69836|  2      |\n",
      "|1    |43 |0    |F.Filtering less-than-mu disks...   |100.0  |5  |3    |0.593 |23121|  2      |\n",
      "|1    |44 |0    |G.Prunning duplicate candidates...  |100.0  |5  |3    |2.936 |13638|  2      |\n",
      "|1    |45 |0    |H.Indexing candidates...            |100.0  |5  |3    |3.364 |13638|  2      |\n",
      "|1    |46 |0    |I.Getting expansions...             |100.0  |5  |3    |0.322 |38445|  2      |\n",
      "+-----+---+-----+------------------------------------+-------+---+-----+------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mdf2 = mdf.join(indexMdf, \"n\")\n",
    "    .join(mdfInfo, \"mdfID\")\n",
    "    .orderBy(\"n\")\n",
    "\n",
    "mdf2.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "null"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = mdf2.collect.map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${folder}ml_${prefix}_mdf.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
