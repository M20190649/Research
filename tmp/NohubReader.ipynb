{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@c971261"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  .master(\"local[*]\") // change to \"yarn-client\" on YARN\n",
    "  .config(\"spark.executor.memory\", \"6g\")\n",
    "  .appName(\"NohupReader\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfilename\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/and/Documents/PhD/Research/Scripts/Python/test.out\"\u001b[39m\n",
       "\u001b[36mnohup\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mString\u001b[39m] = [value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filename = \"/home/and/Documents/PhD/Research/Scripts/Python/test.out\"\n",
    "val nohup = spark.read.textFile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mLine\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mParam\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mStat\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Line(line: String, n: Long)\n",
    "case class Param(runID: Long, date: String, method: String, cores: Int, epsilon: Double, mu: Int, delta: Int, methodTime: Double)\n",
    "case class Stat(runID: Long, n: Long, timestamp: String, stage: String, stageTime: Double, load: Int, unit: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._ \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mlines\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mLine\u001b[39m] = [line: string, n: bigint]\n",
       "\u001b[36mnLines\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2682L\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "import spark.implicits._\n",
    "\n",
    "val lines = nohup.toDF(\"line\").withColumn(\"n\", monotonicallyIncreasingId).as[Line].cache()\n",
    "val nLines = lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mindices\u001b[39m: \u001b[32mList\u001b[39m[((\u001b[32mLong\u001b[39m, \u001b[32mLong\u001b[39m), \u001b[32mInt\u001b[39m)] = \u001b[33mList\u001b[39m(((\u001b[32m26L\u001b[39m, \u001b[32m659L\u001b[39m), \u001b[32m0\u001b[39m), ((\u001b[32m907L\u001b[39m, \u001b[32m1540L\u001b[39m), \u001b[32m1\u001b[39m), ((\u001b[32m1788L\u001b[39m, \u001b[32m2421L\u001b[39m), \u001b[32m2\u001b[39m))\n",
       "\u001b[36mindex\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [runID: int, n: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indices = lines.filter{ l => \n",
    "        l.line.contains(\"=== MergeLast Start ===\") || l.line.contains(\"method=MergeLast,\")\n",
    "    }\n",
    "    .orderBy(\"n\")\n",
    "    .select(\"n\")\n",
    "    .collect()\n",
    "    .toList\n",
    "    .map(_.getLong(0))\n",
    "    .grouped(2)\n",
    "    .toList\n",
    "    .map(pair => (pair.head, pair.last))\n",
    "    .filter(r => r._1 != r._2)\n",
    "    .zipWithIndex\n",
    "val index = spark.createDataset(indices)\n",
    "    .flatMap{ pair => \n",
    "        (pair._1._1 to pair._1._2)\n",
    "        .toList.map(v => (pair._2, v))\n",
    "    }\n",
    "    .toDF(\"runID\",\"n\")\n",
    "    .cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mruns\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [n: bigint, runID: int ... 1 more field]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val runs = index.join(lines, \"n\").cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|runID|date                   |method   |cores|epsilon|mu |delta|methodTime|\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "|0    |2018-05-12 09:34:11,137|MergeLast|28   |10.0   |4  |5    |1431.151  |\n",
      "|1    |2018-05-12 10:12:40,777|MergeLast|28   |20.0   |4  |5    |1851.144  |\n",
      "|2    |2018-05-12 11:05:19,245|MergeLast|28   |30.0   |4  |5    |2484.781  |\n",
      "+-----+-----------------------+---------+-----+-------+---+-----+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mparams\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mParam\u001b[39m] = [runID: bigint, date: string ... 6 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val params = runs.groupBy(\"runID\")\n",
    "    .agg(max($\"n\").alias(\"n\"))\n",
    "    .join(lines, \"n\")\n",
    "    .select(\"runID\", \"line\")\n",
    "    .orderBy(\"runID\")\n",
    "    .map{ row =>\n",
    "        val runID = row.getInt(0)\n",
    "        val line  = row.getString(1)\n",
    "        var arr1  = line.split(\" -> \")\n",
    "        val date  = arr1(0)\n",
    "        val arr2  = arr1(1).split(\",\")\n",
    "        val method  = arr2(0).split(\"=\")(1)\n",
    "        val cores   = arr2(1).split(\"=\")(1).toInt\n",
    "        val epsilon = arr2(2).split(\"=\")(1).toDouble\n",
    "        val mu      = arr2(3).split(\"=\")(1).toInt\n",
    "        val delta   = arr2(4).split(\"=\")(1).toInt\n",
    "        val time    = arr2(5).split(\"=\")(1).toDouble\n",
    "        Param(runID, date, method, cores, epsilon, mu, delta, time)\n",
    "    }\n",
    "    .cache\n",
    "params.show(10, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------------------+------------------------------+---------+----+------------+\n",
      "|runID|n  |timestamp              |stage                         |stageTime|load|unit        |\n",
      "+-----+---+-----------------------+------------------------------+---------+----+------------+\n",
      "|0    |57 |2018-05-12 09:13:01,801|1.Getting disks               |161.63   |30  |disks       |\n",
      "|0    |58 |2018-05-12 09:13:05,945|2.Joining timestams           |4.14     |10  |candidates  |\n",
      "|0    |77 |2018-05-12 09:13:22,486|3.Distance Join phase...      |3.87     |13  |combinations|\n",
      "|0    |78 |2018-05-12 09:13:24,676|4.Getting candidates...       |2.19     |10  |candidates  |\n",
      "|0    |98 |2018-05-12 09:13:42,017|3.Distance Join phase...      |4.06     |15  |combinations|\n",
      "|0    |99 |2018-05-12 09:13:44,276|4.Getting candidates...       |2.26     |10  |candidates  |\n",
      "|0    |119|2018-05-12 09:14:00,637|3.Distance Join phase...      |4.28     |18  |combinations|\n",
      "|0    |120|2018-05-12 09:14:03,010|4.Getting candidates...       |2.37     |10  |candidates  |\n",
      "|0    |122|2018-05-12 09:14:03,970|5.Checking internal timestamps|3.33     |10  |flocks      |\n",
      "|0    |153|2018-05-12 09:16:44,108|1.Getting disks               |159.8    |27  |disks       |\n",
      "+-----+---+-----------------------+------------------------------+---------+----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mstats\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mStat\u001b[39m] = [runID: bigint, n: bigint ... 5 more fields]\n",
       "\u001b[36mres10_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m192L\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stats = runs.filter(_.getString(2).contains(\"|\"))\n",
    "    .map{ row =>\n",
    "        val n     = row.getLong(0)\n",
    "        val runID = row.getInt(1)\n",
    "        val line  = row.getString(2)\n",
    "        var arr1  = line.split(\" -> \")\n",
    "        val timestamp  = arr1(0).trim\n",
    "        val arr2  = arr1(1).split(\"\\\\|\")\n",
    "        val stage = arr2(0).trim\n",
    "        val time  = arr2(1).trim.dropRight(1).toDouble\n",
    "        val arr3  = arr2(2).trim.split(\" \")\n",
    "        val load  = arr3(0).toInt\n",
    "        val unit  = arr3(1)\n",
    "        Stat(runID, n, timestamp, stage, time, load, unit)\n",
    "    }.cache\n",
    "stats.count()\n",
    "stats.show(10, truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------+-----+-------+---+-----+----------+---+--------------------+--------------------+---------+----+------------+\n",
      "|runID|                date|   method|cores|epsilon| mu|delta|methodTime|  n|           timestamp|               stage|stageTime|load|        unit|\n",
      "+-----+--------------------+---------+-----+-------+---+-----+----------+---+--------------------+--------------------+---------+----+------------+\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151| 57|2018-05-12 09:13:...|     1.Getting disks|   161.63|  30|       disks|\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151| 58|2018-05-12 09:13:...| 2.Joining timestams|     4.14|  10|  candidates|\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151| 77|2018-05-12 09:13:...|3.Distance Join p...|     3.87|  13|combinations|\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151| 78|2018-05-12 09:13:...|4.Getting candida...|     2.19|  10|  candidates|\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151| 98|2018-05-12 09:13:...|3.Distance Join p...|     4.06|  15|combinations|\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151| 99|2018-05-12 09:13:...|4.Getting candida...|     2.26|  10|  candidates|\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151|119|2018-05-12 09:14:...|3.Distance Join p...|     4.28|  18|combinations|\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151|120|2018-05-12 09:14:...|4.Getting candida...|     2.37|  10|  candidates|\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151|122|2018-05-12 09:14:...|5.Checking intern...|     3.33|  10|      flocks|\n",
      "|    0|2018-05-12 09:34:...|MergeLast|   28|   10.0|  4|    5|  1431.151|153|2018-05-12 09:16:...|     1.Getting disks|    159.8|  27|       disks|\n",
      "+-----+--------------------+---------+-----+-------+---+-----+----------+---+--------------------+--------------------+---------+----+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [runID: bigint, date: string ... 12 more fields]\n",
       "\u001b[36mres13_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m192L\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = params.join(stats, \"runID\").orderBy(\"n\").cache\n",
    "data.count()\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36md\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"\"\"\n",
       "0;2018-05-12 09:34:11,137;MergeLast;28;10.0;4;5;1431.151;57;2018-05-12 09:13:01,801;1.Getting disks;161.63;30;disks\n",
       "0;2018-05-12 09:34:11,137;MergeLast;28;10.0;4;5;1431.151;58;2018-05-12 09:13:05,945;2.Joining timestams;4.14;10;candidates\n",
       "0;2018-05-12 09:34:11,137;MergeLast;28;10.0;4;5;1431.151;77;2018-05-12 09:13:22,486;3.Distance Join phase...;3.87;13;combinations\n",
       "0;2018-05-12 09:34:11,137;MergeLast;28;10.0;4;5;1431.151;78;2018-05-12 09:13:24,676;4.Getting candidates...;2.19;10;candidates\n",
       "0;2018-05-12 09:34:11,137;MergeLast;28;10.0;4;5;1431.151;98;2018-05-12 09:13:42,017;3.Distance Join phase...;4.06;15;combinations\n",
       "0;2018-05-12 09:34:11,137;MergeLast;28;10.0;4;5;1431.151;99;2018-05-12 09:13:44,\u001b[33m...\u001b[39m\n",
       "\u001b[36mresearch_home\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/and/Documents/PhD/Research/\"\u001b[39m\n",
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"Experiments/Logs/\"\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\u001b[39m\n",
       "\u001b[36mpw\u001b[39m: \u001b[32mPrintWriter\u001b[39m = java.io.PrintWriter@5d68fd21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d = data.collect.sortBy(_.getLong(8)).map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "val research_home: String = scala.util.Properties.envOrElse(\"RESEARCH_HOME\", \"/home/and/Documents/PhD/Research/\")\n",
    "val path = \"Experiments/Logs/\"\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${research_home}${path}output.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
