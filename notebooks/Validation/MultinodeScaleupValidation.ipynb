{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "//import $ivy.`sh.almond::almond-spark:0.3\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                            \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4a98fa02\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1555960667337\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "//import $ivy.`sh.almond::almond-spark:0.3\n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "import $ivy.`org.vegas-viz::vegas:0.3.10`\n",
    "\n",
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql._\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .config(\"spark.serializer\",classOf[KryoSerializer].getName)\n",
    "    .master(\"local[*]\").appName(\"Areal\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mLog\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mLog\u001b[39m] = [timestamp: string, duration: int ... 9 more fields]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Log(timestamp: String, duration: Int, appID: String, nodes: String, id: Int, worker: String, stage: String\n",
    "               , rddBlocks: Int, tasksCompleted: Int, nodeTime: Double , tasksLoad: Double)\n",
    "val data = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"|\")\n",
    "    .csv(\"/home/acald013/Research/Validation/MultinodeScaleup/monitor.log\")\n",
    "    .map{ l => \n",
    "        val timestamp = l.getString(0)\n",
    "        val duration = l.getString(1).toInt\n",
    "        val appID    = l.getString(2)\n",
    "        val nodes    = l.getString(3).toInt\n",
    "        val id       = l.getString(4).toInt\n",
    "        val worker   = l.getString(5)\n",
    "        val stage    = l.getString(6)\n",
    "        val rdds     = l.getString(7).toInt\n",
    "        val comp     = l.getString(8).toInt\n",
    "        val dura     = l.getString(9).toDouble\n",
    "        val load     = l.getString(10).toDouble\n",
    "        \n",
    "        (timestamp, duration, appID, nodes, id, worker, stage, rdds, comp, dura, load)\n",
    "    }.toDF(\"timestamp\", \"duration\", \"appID\", \"nodes\", \"id\", \"worker\", \"stage\", \"rddBlocks\", \"tasksCompleted\", \"nodeTime\", \"tasksLoad\").as[Log]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------+-----------------------+-----+---+--------------------+-----------------------+---------+--------------+--------+---------+\n",
      "|timestamp              |duration|appID                  |nodes|id |worker              |stage                  |rddBlocks|tasksCompleted|nodeTime|tasksLoad|\n",
      "+-----------------------+--------+-----------------------+-----+---+--------------------+-----------------------+---------+--------------+--------+---------+\n",
      "|2019-04-20 11:52:54,707|33      |app-20190420115223-0000|3    |2  |169.235.27.135:33775|flatMap at MF.scala:153|1037     |1354          |20.96   |519.73   |\n",
      "|2019-04-20 11:52:54,707|33      |app-20190420115223-0000|3    |1  |169.235.27.137:45856|flatMap at MF.scala:153|1056     |1358          |21.63   |556.5    |\n",
      "|2019-04-20 11:52:54,707|33      |app-20190420115223-0000|3    |0  |169.235.27.134:40730|flatMap at MF.scala:153|1139     |1513          |24.74   |658.93   |\n",
      "|2019-04-20 11:52:54,967|33      |app-20190420115223-0000|3    |2  |169.235.27.135:33775|count at MF.scala:158  |1129     |1446          |21.34   |519.73   |\n",
      "|2019-04-20 11:52:54,967|33      |app-20190420115223-0000|3    |1  |169.235.27.137:45856|count at MF.scala:158  |1139     |1441          |22.05   |556.5    |\n",
      "|2019-04-20 11:52:54,967|33      |app-20190420115223-0000|3    |0  |169.235.27.134:40730|count at MF.scala:158  |1232     |1606          |25.12   |658.93   |\n",
      "|2019-04-20 11:52:55,178|33      |app-20190420115223-0000|3    |2  |169.235.27.135:33775|count at MF.scala:158  |1165     |1482          |21.57   |519.73   |\n",
      "|2019-04-20 11:52:55,178|33      |app-20190420115223-0000|3    |1  |169.235.27.137:45856|count at MF.scala:158  |1176     |1479          |22.26   |556.5    |\n",
      "|2019-04-20 11:52:55,178|33      |app-20190420115223-0000|3    |0  |169.235.27.134:40730|count at MF.scala:158  |1259     |1633          |25.32   |658.93   |\n",
      "|2019-04-20 11:52:55,419|33      |app-20190420115223-0000|3    |2  |169.235.27.135:33775|count at MF.scala:158  |1187     |1504          |21.77   |519.73   |\n",
      "|2019-04-20 11:52:55,419|33      |app-20190420115223-0000|3    |1  |169.235.27.137:45856|count at MF.scala:158  |1211     |1513          |22.46   |556.5    |\n",
      "|2019-04-20 11:52:55,420|33      |app-20190420115223-0000|3    |0  |169.235.27.134:40730|count at MF.scala:158  |1290     |1664          |25.55   |658.93   |\n",
      "|2019-04-20 11:52:55,671|34      |app-20190420115223-0000|3    |2  |169.235.27.135:33775|count at MF.scala:158  |1250     |1566          |22.1    |519.73   |\n",
      "|2019-04-20 11:52:55,672|34      |app-20190420115223-0000|3    |1  |169.235.27.137:45856|count at MF.scala:158  |1281     |1583          |22.78   |556.5    |\n",
      "|2019-04-20 11:52:55,672|34      |app-20190420115223-0000|3    |0  |169.235.27.134:40730|count at MF.scala:158  |1355     |1727          |25.87   |658.93   |\n",
      "|2019-04-20 11:52:55,954|34      |app-20190420115223-0000|3    |2  |169.235.27.135:33775|count at MF.scala:158  |1321     |1636          |22.34   |519.73   |\n",
      "|2019-04-20 11:52:55,955|34      |app-20190420115223-0000|3    |1  |169.235.27.137:45856|count at MF.scala:158  |1364     |1665          |23.09   |556.5    |\n",
      "|2019-04-20 11:52:55,955|34      |app-20190420115223-0000|3    |0  |169.235.27.134:40730|count at MF.scala:158  |1435     |1809          |26.11   |658.93   |\n",
      "|2019-04-20 11:52:56,278|34      |app-20190420115223-0000|3    |2  |169.235.27.135:33775|count at MF.scala:158  |1418     |1735          |22.7    |519.73   |\n",
      "|2019-04-20 11:52:56,278|34      |app-20190420115223-0000|3    |1  |169.235.27.137:45856|count at MF.scala:158  |1458     |1760          |23.4    |556.5    |\n",
      "+-----------------------+--------+-----------------------+-----+---+--------------------+-----------------------+---------+--------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres33_0\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mLog\u001b[39m] = [timestamp: string, duration: int ... 9 more fields]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.persist(StorageLevel.MEMORY_ONLY)\n",
    "data.show(truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|appID                  |\n",
      "+-----------------------+\n",
      "|app-20190420115223-0000|\n",
      "|app-20190420120928-0001|\n",
      "|app-20190420122227-0002|\n",
      "|app-20190420123748-0003|\n",
      "|app-20190420125022-0004|\n",
      "|app-20190420130734-0005|\n",
      "|app-20190420132031-0006|\n",
      "|app-20190420133750-0007|\n",
      "|app-20190420135120-0008|\n",
      "|app-20190420140717-0009|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mapps\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [appID: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val apps = data.select(\"appID\").distinct().orderBy(\"appID\").cache()\n",
    "apps.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mextractAppRates\u001b[39m"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractAppRates(appID: String, data: Dataset[Log]): DataFrame = {\n",
    "    val node_info = data.filter(_.appID == appID).select($\"duration\", $\"id\", $\"worker\", $\"rddBlocks\", $\"tasksCompleted\", $\"tasksLoad\")\n",
    "        .groupBy($\"duration\", $\"id\", $\"worker\")\n",
    "        .agg(mean($\"rddBlocks\").as(\"rdds\"), mean($\"tasksCompleted\").as(\"task\"), mean($\"tasksLoad\").as(\"load\"))\n",
    "        .withColumn(\"rddsRate\", $\"rdds\" / $\"duration\")\n",
    "        .withColumn(\"taskRate\", $\"task\" / $\"duration\")\n",
    "        .withColumn(\"loadRate\", $\"load\" / $\"duration\")\n",
    "        .select($\"duration\", $\"worker\", $\"rddsRate\", $\"taskRate\", $\"loadRate\")\n",
    "        .orderBy($\"duration\", $\"worker\")\n",
    "    val pw = new PrintWriter(s\"/tmp/${appID}.tsv\")\n",
    "    pw.write(node_info.map{ m => \n",
    "        s\"${m.getInt(0)}\\t${m.getString(1)}\\t${m.getDouble(2)}\\t${m.getDouble(3)}\\t${m.getDouble(4)}\\n\"\n",
    "    }.collect().mkString(\"\"))\n",
    "    pw.close()\n",
    "    \n",
    "    node_info\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------------------+--------------------+\n",
      "|duration|worker              |rddsRate            |taskRate           |loadRate            |\n",
      "+--------+--------------------+--------------------+-------------------+--------------------+\n",
      "|3       |169.235.27.134:40431|0.0                 |0.0                |0.0                 |\n",
      "|4       |169.235.27.134:40431|0.0                 |0.0                |0.0                 |\n",
      "|5       |169.235.27.134:40431|0.0                 |0.03333333333333333|0.0                 |\n",
      "|6       |169.235.27.134:40431|0.0                 |0.25396825396825395|0.050873015873015875|\n",
      "|7       |169.235.27.134:40431|0.006211180124223602|2.3540372670807455 |1.4332919254658378  |\n",
      "|8       |169.235.27.134:40431|1.5773809523809523  |4.738095238095238  |2.8251190476190464  |\n",
      "|9       |169.235.27.134:40431|3.587301587301587   |6.412698412698413  |4.258201058201059   |\n",
      "|10      |169.235.27.134:40431|4.747368421052632   |9.042105263157895  |6.840631578947369   |\n",
      "|11      |169.235.27.134:40431|4.790909090909091   |11.10909090909091  |8.626045454545455   |\n",
      "|12      |169.235.27.134:40431|6.504166666666666   |11.525             |8.287416666666667   |\n",
      "|13      |169.235.27.134:40431|7.932126696832579   |15.176470588235293 |21.64117647058824   |\n",
      "|14      |169.235.27.134:40431|8.869047619047619   |19.29365079365079  |28.048055555555568  |\n",
      "|15      |169.235.27.134:40431|8.65925925925926    |19.9037037037037   |27.19877777777778   |\n",
      "|16      |169.235.27.134:40431|8.466911764705882   |18.996323529411764 |26.286654411764715  |\n",
      "|17      |169.235.27.134:40431|8.30718954248366    |18.18300653594771  |25.723039215686278  |\n",
      "|18      |169.235.27.134:40431|7.944444444444445   |17.416666666666668 |25.111666666666675  |\n",
      "|19      |169.235.27.134:40431|7.526315789473684   |17.05263157894737  |23.79000000000001   |\n",
      "|20      |169.235.27.134:40431|7.15                |16.852777777777778 |22.600500000000007  |\n",
      "|21      |169.235.27.134:40431|7.345238095238095   |17.767857142857142 |22.192976190476195  |\n",
      "|22      |169.235.27.134:40431|7.412878787878788   |20.598484848484848 |25.424507575757573  |\n",
      "+--------+--------------------+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"app-20190420123748-0003\"\u001b[39m\n",
       "\u001b[36mtm\u001b[39m: \u001b[32mDataFrame\u001b[39m = [duration: int, worker: string ... 3 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val appID = \"app-20190420123748-0003\"\n",
    "val tm = extractAppRates(appID, data)\n",
    "tm.show(truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(app <- apps.collect()){\n",
    "    extractAppRates(app.getString(0), data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mextractAppMetrics\u001b[39m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extractAppMetrics(appID: String, data: Dataset[Log]): DataFrame = {\n",
    "    val node_info = data.filter(f => f.appID == appID && f.worker.contains(\":\"))\n",
    "        .select($\"duration\", $\"worker\", $\"rddBlocks\", $\"tasksCompleted\", $\"tasksLoad\")\n",
    "        .groupBy($\"duration\", $\"worker\")\n",
    "        .agg(max($\"rddBlocks\").as(\"rdds\"), max($\"tasksCompleted\").as(\"task\"), max($\"tasksLoad\").as(\"load\"))\n",
    "        .select($\"duration\", $\"worker\", $\"rdds\", $\"task\", $\"load\")\n",
    "        .orderBy($\"duration\", $\"worker\")\n",
    "    val fileID = appID.split(\"-\")(2)\n",
    "    val pw = new PrintWriter(s\"/tmp/M${fileID}.tsv\")\n",
    "    pw.write(node_info.map{ m => \n",
    "        s\"${m.getInt(0)}\\t${m.getString(1)}\\t${m.getInt(2)}\\t${m.getInt(3)}\\t${m.getDouble(4)}\\n\"\n",
    "    }.collect().mkString(\"\"))\n",
    "    pw.close()\n",
    "    \n",
    "    node_info\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for(app <- apps.collect()){\n",
    "    extractAppMetrics(app.getString(0), data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+----------+\n",
      "|metric|               appID|               epoch|       ts0|\n",
      "+------+--------------------+--------------------+----------+\n",
      "| M0000|app-2019042011522...|2019-04-20 11:52:...|1555786341|\n",
      "| M0001|app-2019042012092...|2019-04-20 12:09:...|1555787367|\n",
      "| M0002|app-2019042012222...|2019-04-20 12:22:...|1555788145|\n",
      "| M0003|app-2019042012374...|2019-04-20 12:37:...|1555789066|\n",
      "| M0004|app-2019042012502...|2019-04-20 12:50:...|1555789820|\n",
      "| M0005|app-2019042013073...|2019-04-20 13:07:...|1555790852|\n",
      "| M0006|app-2019042013203...|2019-04-20 13:20:...|1555791630|\n",
      "| M0007|app-2019042013375...|2019-04-20 13:37:...|1555792669|\n",
      "| M0008|app-2019042013512...|2019-04-20 13:51:...|1555793479|\n",
      "| M0009|app-2019042014071...|2019-04-20 14:07:...|1555794435|\n",
      "+------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mstarttimes\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [metric: string, appID: string ... 2 more fields]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val starttimes = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(\"/home/acald013/Research/Validation/starttimes.csv\")\n",
    "    .toDF(\"metric\", \"appID\",\"epoch\").withColumn(\"ts0\", unix_timestamp($\"epoch\", \"yyyy-MM-dd HH:mm:ss\")).cache().cache()\n",
    "starttimes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+--------------------------------------------------+------+------+---------+----------+\n",
      "|epoch                  |appID                  |stage                                             |time  |load  |timestamp|ts1       |\n",
      "+-----------------------+-----------------------+--------------------------------------------------+------+------+---------+----------+\n",
      "|2019-04-20 11:53:44,394|app-20190420115223-0000|1.Maximal disks found                             | 72.34| 39886|0        |1555786424|\n",
      "|2019-04-20 11:54:39,949|app-20190420115223-0000|1.Maximal disks found                             | 55.33| 40330|1        |1555786479|\n",
      "|2019-04-20 11:54:48,052|app-20190420115223-0000|2.Join done                                       |  8.10|128740|1        |1555786488|\n",
      "|2019-04-20 11:54:49,418|app-20190420115223-0000|3.Flocks to report                                |  1.37|     0|1        |1555786489|\n",
      "|2019-04-20 11:54:49,478|app-20190420115223-0000|4.Flocks prunned by expansion                     |  0.06|     0|1        |1555786489|\n",
      "|2019-04-20 11:54:49,485|app-20190420115223-0000|5.Flocks reported                                 |  0.01|     0|1        |1555786489|\n",
      "|2019-04-20 11:54:58,174|app-20190420115223-0000|6.Candidates indexed                              |  8.69|132352|1        |1555786498|\n",
      "|2019-04-20 11:56:13,006|app-20190420115223-0000|1.Maximal disks found                             | 74.83| 41224|2        |1555786573|\n",
      "|2019-04-20 11:57:03,208|app-20190420115223-0000|2.Join done                                       | 50.20|264490|2        |1555786623|\n",
      "|2019-04-20 11:57:08,383|app-20190420115223-0000|3.Flocks to report                                |  5.18|226312|2        |1555786628|\n",
      "|2019-04-20 11:57:58,465|app-20190420115223-0000|4.Flocks prunned by expansion                     | 50.08| 39610|2        |1555786678|\n",
      "|2019-04-20 11:57:59,277|app-20190420115223-0000|5.Flocks reported                                 |  0.81| 39610|2        |1555786679|\n",
      "|2019-04-20 11:58:12,505|app-20190420115223-0000|6.Candidates indexed                              | 13.23| 81040|2        |1555786692|\n",
      "|2019-04-20 11:59:33,232|app-20190420115223-0000|1.Maximal disks found                             | 80.73| 41968|3        |1555786773|\n",
      "|2019-04-20 12:00:36,856|app-20190420115223-0000|2.Join done                                       | 63.62|263266|3        |1555786836|\n",
      "|2019-04-20 12:00:46,790|app-20190420115223-0000|3.Flocks to report                                |  9.93|226198|3        |1555786846|\n",
      "|2019-04-20 12:02:06,804|app-20190420115223-0000|4.Flocks prunned by expansion                     | 80.01| 40018|3        |1555786926|\n",
      "|2019-04-20 12:02:07,968|app-20190420115223-0000|5.Flocks reported                                 |  1.16| 40018|3        |1555786927|\n",
      "|2019-04-20 12:02:29,634|app-20190420115223-0000|6.Candidates indexed                              | 21.67| 80908|3        |1555786949|\n",
      "|2019-04-20 12:03:42,264|app-20190420115223-0000|1.Maximal disks found                             | 72.63| 41956|4        |1555787022|\n",
      "+-----------------------+-----------------------+--------------------------------------------------+------+------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnohup\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [epoch: string, appID: string ... 5 more fields]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nohup = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"|\").csv(\"/home/acald013/Research/Validation/MultinodeScaleup/stages.psv\")\n",
    "    .toDF(\"epoch\", \"appID\", \"stage\", \"time\", \"load\", \"timestamp\").withColumn(\"ts1\", unix_timestamp($\"epoch\", \"yyyy-MM-dd HH:mm:ss\")).cache()\n",
    "nohup.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+--------------------------------------------------+----------+----------+\n",
      "|appID                  |timestamp|stage                                             |ts1       |ts0       |\n",
      "+-----------------------+---------+--------------------------------------------------+----------+----------+\n",
      "|app-20190420115223-0000|0        |1.Maximal disks found                             |1555786424|1555786341|\n",
      "|app-20190420115223-0000|1        |1.Maximal disks found                             |1555786479|1555786341|\n",
      "|app-20190420115223-0000|1        |2.Join done                                       |1555786488|1555786341|\n",
      "|app-20190420115223-0000|1        |3.Flocks to report                                |1555786489|1555786341|\n",
      "|app-20190420115223-0000|1        |4.Flocks prunned by expansion                     |1555786489|1555786341|\n",
      "|app-20190420115223-0000|1        |5.Flocks reported                                 |1555786489|1555786341|\n",
      "|app-20190420115223-0000|1        |6.Candidates indexed                              |1555786498|1555786341|\n",
      "|app-20190420115223-0000|2        |1.Maximal disks found                             |1555786573|1555786341|\n",
      "|app-20190420115223-0000|2        |2.Join done                                       |1555786623|1555786341|\n",
      "|app-20190420115223-0000|2        |3.Flocks to report                                |1555786628|1555786341|\n",
      "|app-20190420115223-0000|2        |4.Flocks prunned by expansion                     |1555786678|1555786341|\n",
      "|app-20190420115223-0000|2        |5.Flocks reported                                 |1555786679|1555786341|\n",
      "|app-20190420115223-0000|2        |6.Candidates indexed                              |1555786692|1555786341|\n",
      "|app-20190420115223-0000|3        |1.Maximal disks found                             |1555786773|1555786341|\n",
      "|app-20190420115223-0000|3        |2.Join done                                       |1555786836|1555786341|\n",
      "|app-20190420115223-0000|3        |3.Flocks to report                                |1555786846|1555786341|\n",
      "|app-20190420115223-0000|3        |4.Flocks prunned by expansion                     |1555786926|1555786341|\n",
      "|app-20190420115223-0000|3        |5.Flocks reported                                 |1555786927|1555786341|\n",
      "|app-20190420115223-0000|3        |6.Candidates indexed                              |1555786949|1555786341|\n",
      "|app-20190420115223-0000|4        |1.Maximal disks found                             |1555787022|1555786341|\n",
      "+-----------------------+---------+--------------------------------------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mjoined\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [appID: string, timestamp: string ... 3 more fields]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joined = nohup.join(starttimes, \"appID\").select(\"appID\", \"timestamp\", \"stage\", \"ts1\", \"ts0\").cache()\n",
    "joined.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---+----------------------------------------------------+\n",
      "|_1                     |_2 |_3                                                  |\n",
      "+-----------------------+---+----------------------------------------------------+\n",
      "|app-20190420115223-0000|83 |0.1.Maximal disks found                             |\n",
      "|app-20190420115223-0000|138|1.1.Maximal disks found                             |\n",
      "|app-20190420115223-0000|147|1.2.Join done                                       |\n",
      "|app-20190420115223-0000|148|1.3.Flocks to report                                |\n",
      "|app-20190420115223-0000|148|1.4.Flocks prunned by expansion                     |\n",
      "|app-20190420115223-0000|148|1.5.Flocks reported                                 |\n",
      "|app-20190420115223-0000|157|1.6.Candidates indexed                              |\n",
      "|app-20190420115223-0000|232|2.1.Maximal disks found                             |\n",
      "|app-20190420115223-0000|282|2.2.Join done                                       |\n",
      "|app-20190420115223-0000|287|2.3.Flocks to report                                |\n",
      "|app-20190420115223-0000|337|2.4.Flocks prunned by expansion                     |\n",
      "|app-20190420115223-0000|338|2.5.Flocks reported                                 |\n",
      "|app-20190420115223-0000|351|2.6.Candidates indexed                              |\n",
      "|app-20190420115223-0000|432|3.1.Maximal disks found                             |\n",
      "|app-20190420115223-0000|495|3.2.Join done                                       |\n",
      "|app-20190420115223-0000|505|3.3.Flocks to report                                |\n",
      "|app-20190420115223-0000|585|3.4.Flocks prunned by expansion                     |\n",
      "|app-20190420115223-0000|586|3.5.Flocks reported                                 |\n",
      "|app-20190420115223-0000|608|3.6.Candidates indexed                              |\n",
      "|app-20190420115223-0000|681|4.1.Maximal disks found                             |\n",
      "+-----------------------+---+----------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mstages\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m)] = [_1: string, _2: bigint ... 1 more field]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stages = joined.map{ j => \n",
    "    val appID = j.getString(0)\n",
    "    val time = j.getLong(3) - j.getLong(4)\n",
    "    val stage = s\"${j.getString(1)}.${j.getString(2)}\"\n",
    "    (appID, time, stage)\n",
    "}\n",
    "stages.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpw\u001b[39m: \u001b[32mPrintWriter\u001b[39m = java.io.PrintWriter@3412b1ef"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pw = new PrintWriter(s\"/tmp/stages.tsv\")\n",
    "pw.write(stages.map{ m => \n",
    "    s\"${m._1}\\t${m._2}\\t${m._3}\\n\"\n",
    "}.collect().mkString(\"\"))\n",
    "pw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
