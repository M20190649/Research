{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7b7cb5c6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  .master(\"local[*]\") // change to \"yarn-client\" on YARN\n",
    "  .config(\"spark.executor.memory\", \"6g\")\n",
    "  .appName(\"NohupReader\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mresearch_home\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/and/Documents/PhD/Research/\"\u001b[39m\n",
       "\u001b[36mfilename\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/and/Documents/PhD/Research/Scripts/Python/test.out\"\u001b[39m\n",
       "\u001b[36mnohup\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mString\u001b[39m] = [value: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val research_home: String = scala.util.Properties.envOrElse(\"RESEARCH_HOME\", \"/home/and/Documents/PhD/Research/\")\n",
    "val filename = s\"${research_home}Scripts/Python/test.out\"\n",
    "val nohup = spark.read.textFile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mLine\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mParam\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mStat\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Line(line: String, n: Long)\n",
    "case class Param(runID: Long, date: String, method: String, cores: Int, epsilon: Double, mu: Int, delta: Int, methodTime: Double)\n",
    "case class Stat(runID: Long, n: Long, timestamp: String, stage: String, stageTime: Double, load: Int, unit: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "import spark.implicits._\n",
    "\n",
    "val lines = nohup.toDF(\"line\").withColumn(\"n\", monotonicallyIncreasingId).as[Line].cache()\n",
    "val nLines = lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val indices = lines.filter{ l => \n",
    "        l.line.contains(\"=== MergeLast Start ===\") || l.line.contains(\"method=MergeLast,\")\n",
    "    }\n",
    "    .orderBy(\"n\")\n",
    "    .select(\"n\")\n",
    "    .collect()\n",
    "    .toList\n",
    "    .map(_.getLong(0))\n",
    "    .grouped(2)\n",
    "    .toList\n",
    "    .map(pair => (pair.head, pair.last))\n",
    "    .filter(r => r._1 != r._2)\n",
    "    .zipWithIndex\n",
    "val index = spark.createDataset(indices)\n",
    "    .flatMap{ pair => \n",
    "        (pair._1._1 to pair._1._2)\n",
    "        .toList.map(v => (pair._2, v))\n",
    "    }\n",
    "    .toDF(\"runID\",\"n\")\n",
    "    .cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val runs = index.join(lines, \"n\").cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val params = runs.groupBy(\"runID\")\n",
    "    .agg(max($\"n\").alias(\"n\"))\n",
    "    .join(lines, \"n\")\n",
    "    .select(\"runID\", \"line\")\n",
    "    .orderBy(\"runID\")\n",
    "    .map{ row =>\n",
    "        val runID = row.getInt(0)\n",
    "        val line  = row.getString(1)\n",
    "        var arr1  = line.split(\" -> \")\n",
    "        val date  = arr1(0)\n",
    "        val arr2  = arr1(1).split(\",\")\n",
    "        val method  = arr2(0).split(\"=\")(1)\n",
    "        val cores   = arr2(1).split(\"=\")(1).toInt\n",
    "        val epsilon = arr2(2).split(\"=\")(1).toDouble\n",
    "        val mu      = arr2(3).split(\"=\")(1).toInt\n",
    "        val delta   = arr2(4).split(\"=\")(1).toInt\n",
    "        val time    = arr2(5).split(\"=\")(1).toDouble\n",
    "        Param(runID, date, method, cores, epsilon, mu, delta, time)\n",
    "    }\n",
    "    .cache\n",
    "params.show(10, truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val stats = runs.filter(_.getString(2).contains(\"|\"))\n",
    "    .map{ row =>\n",
    "        val n     = row.getLong(0)\n",
    "        val runID = row.getInt(1)\n",
    "        val line  = row.getString(2)\n",
    "        var arr1  = line.split(\" -> \")\n",
    "        val timestamp  = arr1(0).trim\n",
    "        val arr2  = arr1(1).split(\"\\\\|\")\n",
    "        val stage = arr2(0).trim\n",
    "        val time  = arr2(1).trim.dropRight(1).toDouble\n",
    "        val arr3  = arr2(2).trim.split(\" \")\n",
    "        val load  = arr3(0).toInt\n",
    "        val unit  = arr3(1)\n",
    "        Stat(runID, n, timestamp, stage, time, load, unit)\n",
    "    }.cache\n",
    "stats.count()\n",
    "stats.show(10, truncate=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data = params.join(stats, \"runID\").orderBy(\"n\").cache\n",
    "data.count()\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val d = data.collect.sortBy(_.getLong(8)).map(_.mkString(\";\")).mkString(\"\\n\")\n",
    "val path = \"Experiments/Logs/\"\n",
    "\n",
    "import java.io._\n",
    "val pw = new PrintWriter(new File(s\"${research_home}${path}output.csv\" ))\n",
    "pw.write(s\"$d\\n\")\n",
    "pw.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class Record(runID: Long, date: String, method: String, cores: Int, epsilon: Double, mu: Int, delta: Int, methodTime: Double, n: Long, timestamp: String, stage: String, stageTime: Double, load: Int, unit: String)\n",
    "data.as[Record]\n",
    "  .cube(\"method\", \"stage\")\n",
    "  .agg(avg(\"stageTime\").as(\"time\"))\n",
    "  .sort($\"method\".desc_nulls_last, $\"stage\".asc_nulls_last)\n",
    "  .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
